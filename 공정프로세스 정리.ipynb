{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 필수 라이브러리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential # 뼈대\n",
    "from tensorflow.keras.layers import InputLayer,Dense,Activation,LSTM\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 읽어들이기\n",
    "raw_df = pd.read_csv('./data/continuous_factory_process.csv', index_col=\"time_stamp\")\n",
    "#1번 모두 처리함(5~14까지의 컬럼 제거 즉 10개.)\n",
    "raw_df_modify = raw_df.drop(raw_df.filter(regex='Setpoint').columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제출데이터 만들기\n",
    "submission_origin = pd.read_csv('./submission_data.csv', index_col=\"time_stamp\")\n",
    "#제출데이터 전처리\n",
    "submission_data = submission_origin.drop(submission_origin.filter(regex='Setpoint').columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machin ambient~5,stage1,2,firstage 분리\n",
    "raw_df_machine0 = raw_df_modify.filter(regex='AmbientConditions') \n",
    "raw_df_machine1 = raw_df_modify.filter(regex='Machine1') #machine1인 데이터값만 가져옴\n",
    "raw_df_machine2 = raw_df_modify.filter(regex='Machine2') \n",
    "raw_df_machine3 = raw_df_modify.filter(regex='Machine3') \n",
    "raw_df_machine4 = raw_df_modify.filter(regex='Machine4') \n",
    "raw_df_machine5 = raw_df_modify.filter(regex='Machine5') \n",
    "raw_df_stage1 = raw_df_modify.filter(regex='Stage1') \n",
    "raw_df_stage2 = raw_df_modify.filter(regex='Stage2') \n",
    "raw_df_firststage = raw_df_modify.filter(regex='FirstStage') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage1 문제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage1 x데이터 출력\n",
    "X = pd.concat([raw_df_machine0,raw_df_machine1,raw_df_machine2,raw_df_machine3,raw_df_firststage],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage1 y데이터 출력\n",
    "y=raw_df_stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage1 데이터 전처리\n",
    "#특정컬럼제거\n",
    "#X=X.drop(['Machine2.RawMaterial.Property1','Machine2.RawMaterial.Property2','Machine3.MaterialTemperature.U.Actual'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage1 데이터 전처리\n",
    "#스케일링\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(X)\n",
    "df_sclaed_df = pd.DataFrame(df_scaled)\n",
    "X = df_sclaed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage1 훈련 정답데이터 분리\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 50)                10400     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 128)               6528      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 15)                1935      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33423 (130.56 KB)\n",
      "Trainable params: 33423 (130.56 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#stage1 모델만들기\n",
    "model_stage1 = Sequential()\n",
    "model_stage1.add(LSTM(units=50,input_shape=(41,1)))\n",
    "model_stage1.add(Dense(units = 128,activation='relu')) #활성화함수를 안에 같이 써줘도 되고 따로 써줘도 무방함.\n",
    "model_stage1.add(Dense(units = 64,activation='relu'))\n",
    "model_stage1.add(Dense(units = 32,activation='relu'))\n",
    "model_stage1.add(Dense(units = 128,activation='relu'))\n",
    "model_stage1.add(tf.keras.layers.Dropout(0.2)),\n",
    "model_stage1.add(Dense(units=15))\n",
    "model_stage1.summary()\n",
    "model_stage1.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "226/226 [==============================] - 5s 13ms/step - loss: 38.6679 - mse: 38.6679 - val_loss: 11.8307 - val_mse: 11.8307\n",
      "Epoch 2/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 14.5155 - mse: 14.5155 - val_loss: 11.8551 - val_mse: 11.8551\n",
      "Epoch 3/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 13.7762 - mse: 13.7762 - val_loss: 12.0665 - val_mse: 12.0665\n",
      "Epoch 4/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 13.4694 - mse: 13.4694 - val_loss: 11.7857 - val_mse: 11.7857\n",
      "Epoch 5/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 13.4096 - mse: 13.4096 - val_loss: 11.8270 - val_mse: 11.8270\n",
      "Epoch 6/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 13.1931 - mse: 13.1931 - val_loss: 11.9742 - val_mse: 11.9742\n",
      "Epoch 7/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 13.1192 - mse: 13.1192 - val_loss: 11.9012 - val_mse: 11.9012\n",
      "Epoch 8/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 13.0727 - mse: 13.0727 - val_loss: 11.7936 - val_mse: 11.7936\n",
      "Epoch 9/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 13.0390 - mse: 13.0390 - val_loss: 11.9019 - val_mse: 11.9019\n",
      "Epoch 10/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.9606 - mse: 12.9606 - val_loss: 11.9625 - val_mse: 11.9625\n",
      "Epoch 11/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.8976 - mse: 12.8976 - val_loss: 11.8774 - val_mse: 11.8774\n",
      "Epoch 12/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.9341 - mse: 12.9341 - val_loss: 11.9789 - val_mse: 11.9789\n",
      "Epoch 13/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 13.0165 - mse: 13.0165 - val_loss: 12.1188 - val_mse: 12.1188\n",
      "Epoch 14/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.8992 - mse: 12.8992 - val_loss: 11.9104 - val_mse: 11.9104\n",
      "Epoch 15/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 12.9770 - mse: 12.9770 - val_loss: 11.8113 - val_mse: 11.8113\n",
      "Epoch 16/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8911 - mse: 12.8911 - val_loss: 11.7668 - val_mse: 11.7668\n",
      "Epoch 17/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.8151 - mse: 12.8151 - val_loss: 11.9075 - val_mse: 11.9075\n",
      "Epoch 18/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.8757 - mse: 12.8757 - val_loss: 11.8948 - val_mse: 11.8948\n",
      "Epoch 19/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8532 - mse: 12.8532 - val_loss: 11.7675 - val_mse: 11.7675\n",
      "Epoch 20/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 12.8871 - mse: 12.8871 - val_loss: 12.1703 - val_mse: 12.1703\n",
      "Epoch 21/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 12.8512 - mse: 12.8512 - val_loss: 11.7712 - val_mse: 11.7712\n",
      "Epoch 22/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8109 - mse: 12.8109 - val_loss: 11.8194 - val_mse: 11.8194\n",
      "Epoch 23/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 12.8389 - mse: 12.8389 - val_loss: 11.9491 - val_mse: 11.9491\n",
      "Epoch 24/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 12.7925 - mse: 12.7925 - val_loss: 11.9264 - val_mse: 11.9264\n",
      "Epoch 25/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 12.8424 - mse: 12.8424 - val_loss: 11.8810 - val_mse: 11.8810\n",
      "Epoch 26/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.7939 - mse: 12.7939 - val_loss: 11.8393 - val_mse: 11.8393\n",
      "Epoch 27/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.7314 - mse: 12.7314 - val_loss: 11.7740 - val_mse: 11.7740\n",
      "Epoch 28/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.9475 - mse: 12.9475 - val_loss: 11.8589 - val_mse: 11.8589\n",
      "Epoch 29/300\n",
      "226/226 [==============================] - 5s 21ms/step - loss: 12.7816 - mse: 12.7816 - val_loss: 12.0157 - val_mse: 12.0157\n",
      "Epoch 30/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8748 - mse: 12.8748 - val_loss: 11.8210 - val_mse: 11.8210\n",
      "Epoch 31/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.7942 - mse: 12.7942 - val_loss: 11.7712 - val_mse: 11.7712\n",
      "Epoch 32/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.7702 - mse: 12.7702 - val_loss: 11.8231 - val_mse: 11.8231\n",
      "Epoch 33/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 12.7272 - mse: 12.7272 - val_loss: 11.9196 - val_mse: 11.9196\n",
      "Epoch 34/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8201 - mse: 12.8201 - val_loss: 11.8005 - val_mse: 11.8005\n",
      "Epoch 35/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.7216 - mse: 12.7216 - val_loss: 11.7971 - val_mse: 11.7971\n",
      "Epoch 36/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8365 - mse: 12.8365 - val_loss: 11.9617 - val_mse: 11.9617\n",
      "Epoch 37/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.7896 - mse: 12.7896 - val_loss: 11.8356 - val_mse: 11.8356\n",
      "Epoch 38/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.8451 - mse: 12.8451 - val_loss: 11.8125 - val_mse: 11.8125\n",
      "Epoch 39/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.7822 - mse: 12.7822 - val_loss: 11.7688 - val_mse: 11.7688\n",
      "Epoch 40/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.8193 - mse: 12.8193 - val_loss: 11.7542 - val_mse: 11.7542\n",
      "Epoch 41/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.7780 - mse: 12.7780 - val_loss: 12.0945 - val_mse: 12.0945\n",
      "Epoch 42/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.7835 - mse: 12.7835 - val_loss: 11.7902 - val_mse: 11.7902\n",
      "Epoch 43/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 12.7154 - mse: 12.7154 - val_loss: 11.7875 - val_mse: 11.7875\n",
      "Epoch 44/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 12.7310 - mse: 12.7310 - val_loss: 12.0371 - val_mse: 12.0371\n",
      "Epoch 45/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.6740 - mse: 12.6740 - val_loss: 11.8227 - val_mse: 11.8227\n",
      "Epoch 46/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.7310 - mse: 12.7310 - val_loss: 11.7529 - val_mse: 11.7529\n",
      "Epoch 47/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.7475 - mse: 12.7475 - val_loss: 11.7770 - val_mse: 11.7770\n",
      "Epoch 48/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.6932 - mse: 12.6932 - val_loss: 11.7897 - val_mse: 11.7897\n",
      "Epoch 49/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.6827 - mse: 12.6827 - val_loss: 11.7384 - val_mse: 11.7384\n",
      "Epoch 50/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.6893 - mse: 12.6893 - val_loss: 12.1365 - val_mse: 12.1365\n",
      "Epoch 51/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.6813 - mse: 12.6813 - val_loss: 12.0193 - val_mse: 12.0193\n",
      "Epoch 52/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.6567 - mse: 12.6567 - val_loss: 12.1426 - val_mse: 12.1426\n",
      "Epoch 53/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.6863 - mse: 12.6863 - val_loss: 11.7639 - val_mse: 11.7639\n",
      "Epoch 54/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.6144 - mse: 12.6144 - val_loss: 11.6991 - val_mse: 11.6991\n",
      "Epoch 55/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.6322 - mse: 12.6322 - val_loss: 11.8816 - val_mse: 11.8816\n",
      "Epoch 56/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.6591 - mse: 12.6591 - val_loss: 11.8021 - val_mse: 11.8021\n",
      "Epoch 57/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 12.5891 - mse: 12.5891 - val_loss: 11.6550 - val_mse: 11.6550\n",
      "Epoch 58/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.5193 - mse: 12.5193 - val_loss: 11.5912 - val_mse: 11.5912\n",
      "Epoch 59/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 12.4504 - mse: 12.4504 - val_loss: 11.4804 - val_mse: 11.4804\n",
      "Epoch 60/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.3218 - mse: 12.3218 - val_loss: 11.5235 - val_mse: 11.5235\n",
      "Epoch 61/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 12.2755 - mse: 12.2755 - val_loss: 11.4050 - val_mse: 11.4050\n",
      "Epoch 62/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.3048 - mse: 12.3048 - val_loss: 11.3433 - val_mse: 11.3433\n",
      "Epoch 63/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 12.1926 - mse: 12.1926 - val_loss: 11.4087 - val_mse: 11.4087\n",
      "Epoch 64/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 12.1724 - mse: 12.1724 - val_loss: 11.4077 - val_mse: 11.4077\n",
      "Epoch 65/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 12.1008 - mse: 12.1008 - val_loss: 10.9561 - val_mse: 10.9561\n",
      "Epoch 66/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 11.0847 - mse: 11.0847 - val_loss: 9.3167 - val_mse: 9.3167\n",
      "Epoch 67/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 10.3029 - mse: 10.3029 - val_loss: 9.1188 - val_mse: 9.1188\n",
      "Epoch 68/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 10.2172 - mse: 10.2172 - val_loss: 8.8230 - val_mse: 8.8230\n",
      "Epoch 69/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 9.9422 - mse: 9.9422 - val_loss: 8.6339 - val_mse: 8.6339\n",
      "Epoch 70/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 9.8728 - mse: 9.8728 - val_loss: 8.4863 - val_mse: 8.4863\n",
      "Epoch 71/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 9.5081 - mse: 9.5081 - val_loss: 8.5595 - val_mse: 8.5595\n",
      "Epoch 72/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 8.6838 - mse: 8.6838 - val_loss: 7.0162 - val_mse: 7.0162\n",
      "Epoch 73/300\n",
      "226/226 [==============================] - 3s 16ms/step - loss: 8.3948 - mse: 8.3948 - val_loss: 6.8082 - val_mse: 6.8082\n",
      "Epoch 74/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 8.1646 - mse: 8.1646 - val_loss: 6.7726 - val_mse: 6.7726\n",
      "Epoch 75/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 8.0823 - mse: 8.0823 - val_loss: 6.5950 - val_mse: 6.5950\n",
      "Epoch 76/300\n",
      "226/226 [==============================] - 6s 25ms/step - loss: 8.0065 - mse: 8.0065 - val_loss: 6.6889 - val_mse: 6.6889\n",
      "Epoch 77/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 7.9893 - mse: 7.9893 - val_loss: 6.7270 - val_mse: 6.7270\n",
      "Epoch 78/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 7.8756 - mse: 7.8756 - val_loss: 6.4949 - val_mse: 6.4949\n",
      "Epoch 79/300\n",
      "226/226 [==============================] - 5s 21ms/step - loss: 7.7763 - mse: 7.7763 - val_loss: 6.6823 - val_mse: 6.6823\n",
      "Epoch 80/300\n",
      "226/226 [==============================] - 5s 20ms/step - loss: 7.7347 - mse: 7.7347 - val_loss: 6.5249 - val_mse: 6.5249\n",
      "Epoch 81/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.7845 - mse: 7.7845 - val_loss: 6.4888 - val_mse: 6.4888\n",
      "Epoch 82/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.8285 - mse: 7.8285 - val_loss: 6.5109 - val_mse: 6.5109\n",
      "Epoch 83/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 7.6778 - mse: 7.6778 - val_loss: 6.6737 - val_mse: 6.6737\n",
      "Epoch 84/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 7.7464 - mse: 7.7464 - val_loss: 6.4398 - val_mse: 6.4398\n",
      "Epoch 85/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.5265 - mse: 7.5265 - val_loss: 6.4673 - val_mse: 6.4673\n",
      "Epoch 86/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.4950 - mse: 7.4950 - val_loss: 6.1577 - val_mse: 6.1577\n",
      "Epoch 87/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 7.4488 - mse: 7.4488 - val_loss: 6.3130 - val_mse: 6.3130\n",
      "Epoch 88/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 7.4346 - mse: 7.4346 - val_loss: 6.2787 - val_mse: 6.2787\n",
      "Epoch 89/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.3505 - mse: 7.3505 - val_loss: 6.1143 - val_mse: 6.1143\n",
      "Epoch 90/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.3657 - mse: 7.3657 - val_loss: 6.1008 - val_mse: 6.1008\n",
      "Epoch 91/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.3106 - mse: 7.3106 - val_loss: 6.2677 - val_mse: 6.2677\n",
      "Epoch 92/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 7.2114 - mse: 7.2114 - val_loss: 6.2875 - val_mse: 6.2875\n",
      "Epoch 93/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.2431 - mse: 7.2431 - val_loss: 5.9133 - val_mse: 5.9133\n",
      "Epoch 94/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.1967 - mse: 7.1967 - val_loss: 6.1965 - val_mse: 6.1965\n",
      "Epoch 95/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.1630 - mse: 7.1630 - val_loss: 5.9333 - val_mse: 5.9333\n",
      "Epoch 96/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 7.2330 - mse: 7.2330 - val_loss: 6.2817 - val_mse: 6.2817\n",
      "Epoch 97/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.2068 - mse: 7.2068 - val_loss: 6.3851 - val_mse: 6.3851\n",
      "Epoch 98/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.0814 - mse: 7.0814 - val_loss: 6.7346 - val_mse: 6.7346\n",
      "Epoch 99/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 7.1274 - mse: 7.1274 - val_loss: 6.1857 - val_mse: 6.1857\n",
      "Epoch 100/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 7.0275 - mse: 7.0275 - val_loss: 5.8131 - val_mse: 5.8131\n",
      "Epoch 101/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 7.0253 - mse: 7.0253 - val_loss: 5.8595 - val_mse: 5.8595\n",
      "Epoch 102/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 6.9093 - mse: 6.9093 - val_loss: 5.8515 - val_mse: 5.8515\n",
      "Epoch 103/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.9648 - mse: 6.9648 - val_loss: 5.9803 - val_mse: 5.9803\n",
      "Epoch 104/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.9934 - mse: 6.9934 - val_loss: 5.7149 - val_mse: 5.7149\n",
      "Epoch 105/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.9134 - mse: 6.9134 - val_loss: 5.6097 - val_mse: 5.6097\n",
      "Epoch 106/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.8551 - mse: 6.8551 - val_loss: 5.7669 - val_mse: 5.7669\n",
      "Epoch 107/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.8371 - mse: 6.8371 - val_loss: 5.8531 - val_mse: 5.8531\n",
      "Epoch 108/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 6.7470 - mse: 6.7470 - val_loss: 5.7906 - val_mse: 5.7906\n",
      "Epoch 109/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 6.7463 - mse: 6.7463 - val_loss: 5.5937 - val_mse: 5.5937\n",
      "Epoch 110/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 6.6495 - mse: 6.6495 - val_loss: 5.5558 - val_mse: 5.5558\n",
      "Epoch 111/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 6.6737 - mse: 6.6737 - val_loss: 5.8746 - val_mse: 5.8746\n",
      "Epoch 112/300\n",
      "226/226 [==============================] - 5s 24ms/step - loss: 6.5582 - mse: 6.5582 - val_loss: 5.6445 - val_mse: 5.6445\n",
      "Epoch 113/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 6.7447 - mse: 6.7447 - val_loss: 5.5897 - val_mse: 5.5897\n",
      "Epoch 114/300\n",
      "226/226 [==============================] - 5s 21ms/step - loss: 6.6896 - mse: 6.6896 - val_loss: 5.6232 - val_mse: 5.6232\n",
      "Epoch 115/300\n",
      "226/226 [==============================] - 4s 20ms/step - loss: 6.5968 - mse: 6.5968 - val_loss: 5.9574 - val_mse: 5.9574\n",
      "Epoch 116/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.6905 - mse: 6.6905 - val_loss: 5.5037 - val_mse: 5.5037\n",
      "Epoch 117/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.6476 - mse: 6.6476 - val_loss: 5.3760 - val_mse: 5.3760\n",
      "Epoch 118/300\n",
      "226/226 [==============================] - 5s 21ms/step - loss: 6.5176 - mse: 6.5176 - val_loss: 5.4444 - val_mse: 5.4444\n",
      "Epoch 119/300\n",
      "226/226 [==============================] - 5s 20ms/step - loss: 6.5001 - mse: 6.5001 - val_loss: 5.7251 - val_mse: 5.7251\n",
      "Epoch 120/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.5124 - mse: 6.5124 - val_loss: 5.8023 - val_mse: 5.8023\n",
      "Epoch 121/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 6.4846 - mse: 6.4846 - val_loss: 5.3329 - val_mse: 5.3329\n",
      "Epoch 122/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 6.4830 - mse: 6.4830 - val_loss: 5.3712 - val_mse: 5.3712\n",
      "Epoch 123/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.3916 - mse: 6.3916 - val_loss: 5.3339 - val_mse: 5.3339\n",
      "Epoch 124/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 6.2708 - mse: 6.2708 - val_loss: 5.1905 - val_mse: 5.1905\n",
      "Epoch 125/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 6.3184 - mse: 6.3184 - val_loss: 5.3789 - val_mse: 5.3789\n",
      "Epoch 126/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.3134 - mse: 6.3134 - val_loss: 5.3254 - val_mse: 5.3254\n",
      "Epoch 127/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.2600 - mse: 6.2600 - val_loss: 5.2119 - val_mse: 5.2119\n",
      "Epoch 128/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.3948 - mse: 6.3948 - val_loss: 5.0622 - val_mse: 5.0622\n",
      "Epoch 129/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.2790 - mse: 6.2790 - val_loss: 5.4304 - val_mse: 5.4304\n",
      "Epoch 130/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.2150 - mse: 6.2150 - val_loss: 5.1783 - val_mse: 5.1783\n",
      "Epoch 131/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.2559 - mse: 6.2559 - val_loss: 5.2990 - val_mse: 5.2990\n",
      "Epoch 132/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.1496 - mse: 6.1496 - val_loss: 5.0356 - val_mse: 5.0356\n",
      "Epoch 133/300\n",
      "226/226 [==============================] - 5s 22ms/step - loss: 6.0931 - mse: 6.0931 - val_loss: 5.1794 - val_mse: 5.1794\n",
      "Epoch 134/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 6.1758 - mse: 6.1758 - val_loss: 5.0640 - val_mse: 5.0640\n",
      "Epoch 135/300\n",
      "226/226 [==============================] - 5s 20ms/step - loss: 6.1319 - mse: 6.1319 - val_loss: 5.3933 - val_mse: 5.3933\n",
      "Epoch 136/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.0492 - mse: 6.0492 - val_loss: 4.9791 - val_mse: 4.9791\n",
      "Epoch 137/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 6.0305 - mse: 6.0305 - val_loss: 4.8923 - val_mse: 4.8923\n",
      "Epoch 138/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.1273 - mse: 6.1273 - val_loss: 5.1275 - val_mse: 5.1275\n",
      "Epoch 139/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 5.9900 - mse: 5.9900 - val_loss: 5.5461 - val_mse: 5.5461\n",
      "Epoch 140/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 6.0173 - mse: 6.0173 - val_loss: 5.2435 - val_mse: 5.2435\n",
      "Epoch 141/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 6.0274 - mse: 6.0274 - val_loss: 5.1268 - val_mse: 5.1268\n",
      "Epoch 142/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.9828 - mse: 5.9828 - val_loss: 4.9484 - val_mse: 4.9484\n",
      "Epoch 143/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 5.9165 - mse: 5.9165 - val_loss: 5.0035 - val_mse: 5.0035\n",
      "Epoch 144/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 5.7700 - mse: 5.7700 - val_loss: 4.8636 - val_mse: 4.8636\n",
      "Epoch 145/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.8368 - mse: 5.8368 - val_loss: 4.7953 - val_mse: 4.7953\n",
      "Epoch 146/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.8223 - mse: 5.8223 - val_loss: 5.1042 - val_mse: 5.1042\n",
      "Epoch 147/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.8138 - mse: 5.8138 - val_loss: 4.5557 - val_mse: 4.5557\n",
      "Epoch 148/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 5.9043 - mse: 5.9043 - val_loss: 4.9753 - val_mse: 4.9753\n",
      "Epoch 149/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.6884 - mse: 5.6884 - val_loss: 4.6379 - val_mse: 4.6379\n",
      "Epoch 150/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 5.7121 - mse: 5.7121 - val_loss: 5.4347 - val_mse: 5.4347\n",
      "Epoch 151/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.7914 - mse: 5.7914 - val_loss: 4.9348 - val_mse: 4.9348\n",
      "Epoch 152/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.7305 - mse: 5.7305 - val_loss: 4.7258 - val_mse: 4.7258\n",
      "Epoch 153/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 5.6357 - mse: 5.6357 - val_loss: 4.8102 - val_mse: 4.8102\n",
      "Epoch 154/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.7787 - mse: 5.7787 - val_loss: 4.5194 - val_mse: 4.5194\n",
      "Epoch 155/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.5870 - mse: 5.5870 - val_loss: 5.1601 - val_mse: 5.1601\n",
      "Epoch 156/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 5.6832 - mse: 5.6832 - val_loss: 4.8352 - val_mse: 4.8352\n",
      "Epoch 157/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 5.6480 - mse: 5.6480 - val_loss: 5.3827 - val_mse: 5.3827\n",
      "Epoch 158/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 5.7338 - mse: 5.7338 - val_loss: 4.3571 - val_mse: 4.3571\n",
      "Epoch 159/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.5388 - mse: 5.5388 - val_loss: 4.5744 - val_mse: 4.5744\n",
      "Epoch 160/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 5.4609 - mse: 5.4609 - val_loss: 4.6150 - val_mse: 4.6150\n",
      "Epoch 161/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.4416 - mse: 5.4416 - val_loss: 4.5423 - val_mse: 4.5423\n",
      "Epoch 162/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 5.5506 - mse: 5.5506 - val_loss: 4.5278 - val_mse: 4.5278\n",
      "Epoch 163/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.3562 - mse: 5.3562 - val_loss: 4.5246 - val_mse: 4.5246\n",
      "Epoch 164/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.3953 - mse: 5.3953 - val_loss: 4.2593 - val_mse: 4.2593\n",
      "Epoch 165/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.3804 - mse: 5.3804 - val_loss: 4.3432 - val_mse: 4.3432\n",
      "Epoch 166/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 5.2707 - mse: 5.2707 - val_loss: 4.6349 - val_mse: 4.6349\n",
      "Epoch 167/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 5.3279 - mse: 5.3279 - val_loss: 4.5491 - val_mse: 4.5491\n",
      "Epoch 168/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.3141 - mse: 5.3141 - val_loss: 4.6976 - val_mse: 4.6976\n",
      "Epoch 169/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 5.5539 - mse: 5.5539 - val_loss: 4.4245 - val_mse: 4.4245\n",
      "Epoch 170/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.5367 - mse: 5.5367 - val_loss: 4.4044 - val_mse: 4.4044\n",
      "Epoch 171/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.2862 - mse: 5.2862 - val_loss: 4.3809 - val_mse: 4.3809\n",
      "Epoch 172/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 5.2412 - mse: 5.2412 - val_loss: 4.9160 - val_mse: 4.9160\n",
      "Epoch 173/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 5.1471 - mse: 5.1471 - val_loss: 4.3592 - val_mse: 4.3592\n",
      "Epoch 174/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 5.2856 - mse: 5.2856 - val_loss: 4.6122 - val_mse: 4.6122\n",
      "Epoch 175/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 5.1856 - mse: 5.1856 - val_loss: 4.2465 - val_mse: 4.2465\n",
      "Epoch 176/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.0342 - mse: 5.0342 - val_loss: 4.8200 - val_mse: 4.8200\n",
      "Epoch 177/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 5.3489 - mse: 5.3489 - val_loss: 4.8944 - val_mse: 4.8944\n",
      "Epoch 178/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 5.1150 - mse: 5.1150 - val_loss: 4.2516 - val_mse: 4.2516\n",
      "Epoch 179/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 5.1355 - mse: 5.1355 - val_loss: 4.1656 - val_mse: 4.1656\n",
      "Epoch 180/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.0441 - mse: 5.0441 - val_loss: 4.6868 - val_mse: 4.6868\n",
      "Epoch 181/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 5.0319 - mse: 5.0319 - val_loss: 4.3956 - val_mse: 4.3956\n",
      "Epoch 182/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 5.0595 - mse: 5.0595 - val_loss: 4.2210 - val_mse: 4.2210\n",
      "Epoch 183/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.9042 - mse: 4.9042 - val_loss: 4.1577 - val_mse: 4.1577\n",
      "Epoch 184/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.7929 - mse: 4.7929 - val_loss: 4.3826 - val_mse: 4.3826\n",
      "Epoch 185/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.8017 - mse: 4.8017 - val_loss: 4.5860 - val_mse: 4.5860\n",
      "Epoch 186/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.8172 - mse: 4.8172 - val_loss: 4.5238 - val_mse: 4.5238\n",
      "Epoch 187/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 5.0199 - mse: 5.0199 - val_loss: 3.9918 - val_mse: 3.9918\n",
      "Epoch 188/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.8761 - mse: 4.8761 - val_loss: 4.3407 - val_mse: 4.3407\n",
      "Epoch 189/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.7415 - mse: 4.7415 - val_loss: 4.5787 - val_mse: 4.5787\n",
      "Epoch 190/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.6746 - mse: 4.6746 - val_loss: 4.1067 - val_mse: 4.1067\n",
      "Epoch 191/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 4.8073 - mse: 4.8073 - val_loss: 4.3146 - val_mse: 4.3146\n",
      "Epoch 192/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.7542 - mse: 4.7542 - val_loss: 4.2429 - val_mse: 4.2429\n",
      "Epoch 193/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.8399 - mse: 4.8399 - val_loss: 4.0284 - val_mse: 4.0284\n",
      "Epoch 194/300\n",
      "226/226 [==============================] - 4s 19ms/step - loss: 4.6920 - mse: 4.6920 - val_loss: 4.5865 - val_mse: 4.5865\n",
      "Epoch 195/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 4.8321 - mse: 4.8321 - val_loss: 4.1320 - val_mse: 4.1320\n",
      "Epoch 196/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.6649 - mse: 4.6649 - val_loss: 4.3017 - val_mse: 4.3017\n",
      "Epoch 197/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.5169 - mse: 4.5169 - val_loss: 3.8481 - val_mse: 3.8481\n",
      "Epoch 198/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.6520 - mse: 4.6520 - val_loss: 4.1239 - val_mse: 4.1239\n",
      "Epoch 199/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.4998 - mse: 4.4998 - val_loss: 4.1367 - val_mse: 4.1367\n",
      "Epoch 200/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.6489 - mse: 4.6489 - val_loss: 4.1501 - val_mse: 4.1501\n",
      "Epoch 201/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.7780 - mse: 4.7780 - val_loss: 3.7545 - val_mse: 3.7545\n",
      "Epoch 202/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.6716 - mse: 4.6716 - val_loss: 3.8705 - val_mse: 3.8705\n",
      "Epoch 203/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.4338 - mse: 4.4338 - val_loss: 4.1325 - val_mse: 4.1325\n",
      "Epoch 204/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.4101 - mse: 4.4101 - val_loss: 3.7337 - val_mse: 3.7337\n",
      "Epoch 205/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.6242 - mse: 4.6242 - val_loss: 4.1282 - val_mse: 4.1282\n",
      "Epoch 206/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.4725 - mse: 4.4725 - val_loss: 3.9829 - val_mse: 3.9829\n",
      "Epoch 207/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.5672 - mse: 4.5672 - val_loss: 3.9018 - val_mse: 3.9018\n",
      "Epoch 208/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 4.3155 - mse: 4.3155 - val_loss: 3.8863 - val_mse: 3.8863\n",
      "Epoch 209/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.4705 - mse: 4.4705 - val_loss: 4.2230 - val_mse: 4.2230\n",
      "Epoch 210/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.6657 - mse: 4.6657 - val_loss: 4.5010 - val_mse: 4.5010\n",
      "Epoch 211/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.7595 - mse: 4.7595 - val_loss: 3.9513 - val_mse: 3.9513\n",
      "Epoch 212/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.1986 - mse: 4.1986 - val_loss: 3.8505 - val_mse: 3.8505\n",
      "Epoch 213/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.3014 - mse: 4.3014 - val_loss: 4.0603 - val_mse: 4.0603\n",
      "Epoch 214/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.4175 - mse: 4.4175 - val_loss: 4.2992 - val_mse: 4.2992\n",
      "Epoch 215/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.2865 - mse: 4.2865 - val_loss: 3.6013 - val_mse: 3.6013\n",
      "Epoch 216/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.5026 - mse: 4.5026 - val_loss: 3.7371 - val_mse: 3.7371\n",
      "Epoch 217/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.2776 - mse: 4.2776 - val_loss: 3.9356 - val_mse: 3.9356\n",
      "Epoch 218/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.4950 - mse: 4.4950 - val_loss: 4.5131 - val_mse: 4.5131\n",
      "Epoch 219/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.3104 - mse: 4.3104 - val_loss: 3.6768 - val_mse: 3.6768\n",
      "Epoch 220/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 4.2540 - mse: 4.2540 - val_loss: 3.7442 - val_mse: 3.7442\n",
      "Epoch 221/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.3840 - mse: 4.3840 - val_loss: 3.6400 - val_mse: 3.6400\n",
      "Epoch 222/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 4.4137 - mse: 4.4137 - val_loss: 3.8627 - val_mse: 3.8627\n",
      "Epoch 223/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.4654 - mse: 4.4654 - val_loss: 3.9436 - val_mse: 3.9436\n",
      "Epoch 224/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.1364 - mse: 4.1364 - val_loss: 3.8203 - val_mse: 3.8203\n",
      "Epoch 225/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.4239 - mse: 4.4239 - val_loss: 4.0792 - val_mse: 4.0792\n",
      "Epoch 226/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.3223 - mse: 4.3223 - val_loss: 3.6245 - val_mse: 3.6245\n",
      "Epoch 227/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.0635 - mse: 4.0635 - val_loss: 3.6371 - val_mse: 3.6371\n",
      "Epoch 228/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 4.3200 - mse: 4.3200 - val_loss: 3.7458 - val_mse: 3.7458\n",
      "Epoch 229/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.3176 - mse: 4.3176 - val_loss: 4.4253 - val_mse: 4.4253\n",
      "Epoch 230/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.1201 - mse: 4.1201 - val_loss: 3.7685 - val_mse: 3.7685\n",
      "Epoch 231/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 4.1578 - mse: 4.1578 - val_loss: 4.1464 - val_mse: 4.1464\n",
      "Epoch 232/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.1660 - mse: 4.1660 - val_loss: 3.5737 - val_mse: 3.5737\n",
      "Epoch 233/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.0462 - mse: 4.0462 - val_loss: 3.6144 - val_mse: 3.6144\n",
      "Epoch 234/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.2745 - mse: 4.2745 - val_loss: 3.5485 - val_mse: 3.5485\n",
      "Epoch 235/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 4.0003 - mse: 4.0003 - val_loss: 3.5363 - val_mse: 3.5363\n",
      "Epoch 236/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.2147 - mse: 4.2147 - val_loss: 3.8435 - val_mse: 3.8435\n",
      "Epoch 237/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.5052 - mse: 4.5052 - val_loss: 3.7316 - val_mse: 3.7316\n",
      "Epoch 238/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.3980 - mse: 4.3980 - val_loss: 3.9317 - val_mse: 3.9317\n",
      "Epoch 239/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.0900 - mse: 4.0900 - val_loss: 3.5705 - val_mse: 3.5705\n",
      "Epoch 240/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.0078 - mse: 4.0078 - val_loss: 3.5075 - val_mse: 3.5075\n",
      "Epoch 241/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.9250 - mse: 3.9250 - val_loss: 3.9190 - val_mse: 3.9190\n",
      "Epoch 242/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.9853 - mse: 3.9853 - val_loss: 3.5736 - val_mse: 3.5736\n",
      "Epoch 243/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.0612 - mse: 4.0612 - val_loss: 3.7740 - val_mse: 3.7740\n",
      "Epoch 244/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.2963 - mse: 4.2963 - val_loss: 3.9376 - val_mse: 3.9376\n",
      "Epoch 245/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.8775 - mse: 4.8775 - val_loss: 4.0211 - val_mse: 4.0211\n",
      "Epoch 246/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.1169 - mse: 4.1169 - val_loss: 3.6657 - val_mse: 3.6657\n",
      "Epoch 247/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.9448 - mse: 3.9448 - val_loss: 3.7307 - val_mse: 3.7307\n",
      "Epoch 248/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.9719 - mse: 3.9719 - val_loss: 3.6109 - val_mse: 3.6109\n",
      "Epoch 249/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 3.9645 - mse: 3.9645 - val_loss: 3.5723 - val_mse: 3.5723\n",
      "Epoch 250/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 3.8661 - mse: 3.8661 - val_loss: 3.6208 - val_mse: 3.6208\n",
      "Epoch 251/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.9765 - mse: 3.9765 - val_loss: 3.6043 - val_mse: 3.6043\n",
      "Epoch 252/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 3.9473 - mse: 3.9473 - val_loss: 3.7389 - val_mse: 3.7389\n",
      "Epoch 253/300\n",
      "226/226 [==============================] - 4s 17ms/step - loss: 3.9882 - mse: 3.9882 - val_loss: 3.5137 - val_mse: 3.5137\n",
      "Epoch 254/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 3.9259 - mse: 3.9259 - val_loss: 4.0005 - val_mse: 4.0005\n",
      "Epoch 255/300\n",
      "226/226 [==============================] - 4s 18ms/step - loss: 4.4234 - mse: 4.4234 - val_loss: 3.5122 - val_mse: 3.5122\n",
      "Epoch 256/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.9552 - mse: 3.9552 - val_loss: 3.5262 - val_mse: 3.5262\n",
      "Epoch 257/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.9193 - mse: 3.9193 - val_loss: 3.4541 - val_mse: 3.4541\n",
      "Epoch 258/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.1705 - mse: 4.1705 - val_loss: 3.4957 - val_mse: 3.4957\n",
      "Epoch 259/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.9614 - mse: 3.9614 - val_loss: 3.5268 - val_mse: 3.5268\n",
      "Epoch 260/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.8434 - mse: 3.8434 - val_loss: 3.6416 - val_mse: 3.6416\n",
      "Epoch 261/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.9718 - mse: 3.9718 - val_loss: 4.0839 - val_mse: 4.0839\n",
      "Epoch 262/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.8721 - mse: 3.8721 - val_loss: 3.6048 - val_mse: 3.6048\n",
      "Epoch 263/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.3189 - mse: 4.3189 - val_loss: 3.7874 - val_mse: 3.7874\n",
      "Epoch 264/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.8024 - mse: 3.8024 - val_loss: 3.5108 - val_mse: 3.5108\n",
      "Epoch 265/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.7398 - mse: 3.7398 - val_loss: 3.8436 - val_mse: 3.8436\n",
      "Epoch 266/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 4.1381 - mse: 4.1381 - val_loss: 4.5796 - val_mse: 4.5796\n",
      "Epoch 267/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.7415 - mse: 3.7415 - val_loss: 3.4929 - val_mse: 3.4929\n",
      "Epoch 268/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.7770 - mse: 3.7770 - val_loss: 3.4461 - val_mse: 3.4461\n",
      "Epoch 269/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.8937 - mse: 3.8937 - val_loss: 3.7270 - val_mse: 3.7270\n",
      "Epoch 270/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 4.0777 - mse: 4.0777 - val_loss: 4.3838 - val_mse: 4.3838\n",
      "Epoch 271/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.9980 - mse: 3.9980 - val_loss: 3.8700 - val_mse: 3.8700\n",
      "Epoch 272/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.8808 - mse: 3.8808 - val_loss: 3.5966 - val_mse: 3.5966\n",
      "Epoch 273/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.0287 - mse: 4.0287 - val_loss: 3.8941 - val_mse: 3.8941\n",
      "Epoch 274/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 4.0335 - mse: 4.0335 - val_loss: 3.8196 - val_mse: 3.8196\n",
      "Epoch 275/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.0301 - mse: 4.0301 - val_loss: 3.9024 - val_mse: 3.9024\n",
      "Epoch 276/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 4.0374 - mse: 4.0374 - val_loss: 3.7030 - val_mse: 3.7030\n",
      "Epoch 277/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.9120 - mse: 3.9120 - val_loss: 3.6076 - val_mse: 3.6076\n",
      "Epoch 278/300\n",
      "226/226 [==============================] - 3s 15ms/step - loss: 3.6429 - mse: 3.6429 - val_loss: 3.5828 - val_mse: 3.5828\n",
      "Epoch 279/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.8331 - mse: 3.8331 - val_loss: 3.5985 - val_mse: 3.5985\n",
      "Epoch 280/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.8527 - mse: 3.8527 - val_loss: 3.4939 - val_mse: 3.4939\n",
      "Epoch 281/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6866 - mse: 3.6866 - val_loss: 3.5880 - val_mse: 3.5880\n",
      "Epoch 282/300\n",
      "226/226 [==============================] - 4s 16ms/step - loss: 3.9207 - mse: 3.9207 - val_loss: 3.4665 - val_mse: 3.4665\n",
      "Epoch 283/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6631 - mse: 3.6631 - val_loss: 3.3744 - val_mse: 3.3744\n",
      "Epoch 284/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.9183 - mse: 3.9183 - val_loss: 3.8538 - val_mse: 3.8538\n",
      "Epoch 285/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.6934 - mse: 3.6934 - val_loss: 3.3747 - val_mse: 3.3747\n",
      "Epoch 286/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.8574 - mse: 3.8574 - val_loss: 3.5579 - val_mse: 3.5579\n",
      "Epoch 287/300\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 3.8244 - mse: 3.8244 - val_loss: 3.8817 - val_mse: 3.8817\n",
      "Epoch 288/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6172 - mse: 3.6172 - val_loss: 3.4050 - val_mse: 3.4050\n",
      "Epoch 289/300\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.6054 - mse: 3.6054 - val_loss: 3.6470 - val_mse: 3.6470\n",
      "Epoch 290/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.6073 - mse: 3.6073 - val_loss: 3.6010 - val_mse: 3.6010\n",
      "Epoch 291/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.6757 - mse: 3.6757 - val_loss: 3.3968 - val_mse: 3.3968\n",
      "Epoch 292/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.9583 - mse: 3.9583 - val_loss: 3.3167 - val_mse: 3.3167\n",
      "Epoch 293/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.8874 - mse: 3.8874 - val_loss: 4.1961 - val_mse: 4.1961\n",
      "Epoch 294/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.7578 - mse: 3.7578 - val_loss: 3.5132 - val_mse: 3.5132\n",
      "Epoch 295/300\n",
      "226/226 [==============================] - 3s 12ms/step - loss: 3.7042 - mse: 3.7042 - val_loss: 3.3262 - val_mse: 3.3262\n",
      "Epoch 296/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.9512 - mse: 3.9512 - val_loss: 3.8428 - val_mse: 3.8428\n",
      "Epoch 297/300\n",
      "226/226 [==============================] - 3s 14ms/step - loss: 3.6119 - mse: 3.6119 - val_loss: 3.3298 - val_mse: 3.3298\n",
      "Epoch 298/300\n",
      "226/226 [==============================] - 3s 13ms/step - loss: 3.5401 - mse: 3.5401 - val_loss: 3.4139 - val_mse: 3.4139\n",
      "Epoch 299/300\n",
      "226/226 [==============================] - 3s 11ms/step - loss: 4.1366 - mse: 4.1366 - val_loss: 4.3579 - val_mse: 4.3579\n",
      "Epoch 300/300\n",
      "226/226 [==============================] - 2s 11ms/step - loss: 3.8835 - mse: 3.8835 - val_loss: 4.0427 - val_mse: 4.0427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x160a4d21090>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stage1 모델 학습\n",
    "model_stage1.fit(X_train,y_train,validation_split=0.2,  #훈련데이터 내에서 20% 검증데이터로 사용\n",
    "          epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13.044704  , 13.334288  , 11.286854  , ...,  1.5257133 ,\n",
       "         3.1038427 , 14.693251  ],\n",
       "       [12.850613  ,  0.65345573, 11.311343  , ...,  1.4228444 ,\n",
       "         3.353666  ,  1.1356988 ],\n",
       "       [12.850813  ,  9.800038  , 11.573832  , ...,  0.7363977 ,\n",
       "         2.6759791 , 11.646754  ],\n",
       "       ...,\n",
       "       [13.040146  , 13.4546    , 11.273175  , ...,  1.5243726 ,\n",
       "         3.101024  , 14.363474  ],\n",
       "       [12.8120365 ,  7.9161134 , 11.600626  , ...,  0.7106029 ,\n",
       "         2.722569  , 13.520613  ],\n",
       "       [12.887492  ,  0.8125253 , 11.484982  , ...,  1.4860021 ,\n",
       "         3.229618  , 11.463323  ]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stage1 모델 예측\n",
    "model_stage1.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#stage1 모델 정답데이터 만들기\n",
    "#submission의 x데이터 가져오기\n",
    "submission_stage_X = submission_data.iloc[:,0:41] \n",
    "\n",
    "#submission의 x데이터 전처리(모델에 학습했던 colunm selection 과 같게!)\n",
    "#submission_stage_X=submission_stage_X.drop(['Machine2.RawMaterial.Property1','Machine2.RawMaterial.Property2','Machine3.MaterialTemperature.U.Actual'],axis=1)\n",
    "\n",
    "#submission의 x데이터 전처리(stage1의 x데이터 처럼 스케일링!)\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(submission_stage_X)\n",
    "df_sclaed_df = pd.DataFrame(df_scaled)\n",
    "X_submission = df_sclaed_df\n",
    "#stage1 모델 정답데이터 출력\n",
    "model_submission1_pre = model_stage1.predict(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage1 제출하기\n",
    "#submission 데이터에 정답데이터 넣기\n",
    "submission_data.iloc[:,42:57] =  model_submission1_pre\n",
    "#submission1 저장하고 파일로 만들기\n",
    "submission1 = submission_data.iloc[:,42:57]\n",
    "np.save('submission1.npy', submission1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stage1.Output.Measurement1.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement2.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement3.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement4.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement5.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement6.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement7.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement8.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement9.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement10.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement11.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement12.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement13.U.Actual</th>\n",
       "      <th>Stage1.Output.Measurement14.U.Actual</th>\n",
       "      <th>Machine4.Temperature1.C.Actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:34</th>\n",
       "      <td>12.881602</td>\n",
       "      <td>11.108870</td>\n",
       "      <td>11.525728</td>\n",
       "      <td>21.446243</td>\n",
       "      <td>31.974375</td>\n",
       "      <td>0.100744</td>\n",
       "      <td>2.394584</td>\n",
       "      <td>2.968608</td>\n",
       "      <td>21.012835</td>\n",
       "      <td>18.983297</td>\n",
       "      <td>7.777860</td>\n",
       "      <td>4.820940</td>\n",
       "      <td>0.772798</td>\n",
       "      <td>2.772620</td>\n",
       "      <td>14.534842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:35</th>\n",
       "      <td>12.864292</td>\n",
       "      <td>10.796167</td>\n",
       "      <td>11.518107</td>\n",
       "      <td>21.392389</td>\n",
       "      <td>31.814201</td>\n",
       "      <td>0.100739</td>\n",
       "      <td>2.360145</td>\n",
       "      <td>2.881801</td>\n",
       "      <td>20.888096</td>\n",
       "      <td>18.906410</td>\n",
       "      <td>7.756851</td>\n",
       "      <td>4.728960</td>\n",
       "      <td>0.779288</td>\n",
       "      <td>2.769287</td>\n",
       "      <td>14.227365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:36</th>\n",
       "      <td>12.848588</td>\n",
       "      <td>10.807688</td>\n",
       "      <td>11.494731</td>\n",
       "      <td>21.369123</td>\n",
       "      <td>31.795662</td>\n",
       "      <td>0.100813</td>\n",
       "      <td>2.359014</td>\n",
       "      <td>2.890857</td>\n",
       "      <td>20.840380</td>\n",
       "      <td>18.861372</td>\n",
       "      <td>7.753476</td>\n",
       "      <td>4.690063</td>\n",
       "      <td>0.785036</td>\n",
       "      <td>2.769865</td>\n",
       "      <td>14.270016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:37</th>\n",
       "      <td>12.870073</td>\n",
       "      <td>10.957961</td>\n",
       "      <td>11.514869</td>\n",
       "      <td>21.413769</td>\n",
       "      <td>31.891699</td>\n",
       "      <td>0.101447</td>\n",
       "      <td>2.372782</td>\n",
       "      <td>2.920685</td>\n",
       "      <td>20.953377</td>\n",
       "      <td>18.948387</td>\n",
       "      <td>7.767488</td>\n",
       "      <td>4.739079</td>\n",
       "      <td>0.782860</td>\n",
       "      <td>2.774833</td>\n",
       "      <td>14.384173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:39</th>\n",
       "      <td>12.590029</td>\n",
       "      <td>10.428984</td>\n",
       "      <td>11.209401</td>\n",
       "      <td>20.901497</td>\n",
       "      <td>30.987888</td>\n",
       "      <td>0.088306</td>\n",
       "      <td>2.202711</td>\n",
       "      <td>2.635536</td>\n",
       "      <td>19.195074</td>\n",
       "      <td>17.458477</td>\n",
       "      <td>7.608414</td>\n",
       "      <td>4.330048</td>\n",
       "      <td>0.759154</td>\n",
       "      <td>2.627956</td>\n",
       "      <td>13.159143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:46:57</th>\n",
       "      <td>12.849593</td>\n",
       "      <td>1.146642</td>\n",
       "      <td>11.325775</td>\n",
       "      <td>21.487192</td>\n",
       "      <td>33.346756</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>0.332409</td>\n",
       "      <td>-0.012815</td>\n",
       "      <td>20.700445</td>\n",
       "      <td>18.849937</td>\n",
       "      <td>7.647273</td>\n",
       "      <td>0.140641</td>\n",
       "      <td>1.413460</td>\n",
       "      <td>3.295619</td>\n",
       "      <td>2.207462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:09</th>\n",
       "      <td>12.852317</td>\n",
       "      <td>1.153117</td>\n",
       "      <td>11.325607</td>\n",
       "      <td>21.496141</td>\n",
       "      <td>33.379215</td>\n",
       "      <td>0.048376</td>\n",
       "      <td>0.318794</td>\n",
       "      <td>-0.025988</td>\n",
       "      <td>20.714857</td>\n",
       "      <td>18.860575</td>\n",
       "      <td>7.652046</td>\n",
       "      <td>0.118026</td>\n",
       "      <td>1.415275</td>\n",
       "      <td>3.301226</td>\n",
       "      <td>2.097956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:14</th>\n",
       "      <td>12.850616</td>\n",
       "      <td>1.103623</td>\n",
       "      <td>11.322406</td>\n",
       "      <td>21.497688</td>\n",
       "      <td>33.391533</td>\n",
       "      <td>0.046241</td>\n",
       "      <td>0.308263</td>\n",
       "      <td>-0.021213</td>\n",
       "      <td>20.722038</td>\n",
       "      <td>18.868387</td>\n",
       "      <td>7.652266</td>\n",
       "      <td>0.108305</td>\n",
       "      <td>1.415003</td>\n",
       "      <td>3.306916</td>\n",
       "      <td>2.002542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:15</th>\n",
       "      <td>12.852301</td>\n",
       "      <td>1.134105</td>\n",
       "      <td>11.324818</td>\n",
       "      <td>21.499315</td>\n",
       "      <td>33.389832</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.311799</td>\n",
       "      <td>-0.024984</td>\n",
       "      <td>20.720697</td>\n",
       "      <td>18.865589</td>\n",
       "      <td>7.652730</td>\n",
       "      <td>0.111721</td>\n",
       "      <td>1.415589</td>\n",
       "      <td>3.303921</td>\n",
       "      <td>2.039164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:19</th>\n",
       "      <td>12.847159</td>\n",
       "      <td>1.051081</td>\n",
       "      <td>11.318814</td>\n",
       "      <td>21.495878</td>\n",
       "      <td>33.386353</td>\n",
       "      <td>0.043773</td>\n",
       "      <td>0.301311</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>20.722193</td>\n",
       "      <td>18.871586</td>\n",
       "      <td>7.650171</td>\n",
       "      <td>0.109741</td>\n",
       "      <td>1.413550</td>\n",
       "      <td>3.310215</td>\n",
       "      <td>1.928090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2818 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Stage1.Output.Measurement1.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            12.881602   \n",
       "2019-03-06 10:52:35                            12.864292   \n",
       "2019-03-06 10:52:36                            12.848588   \n",
       "2019-03-06 10:52:37                            12.870073   \n",
       "2019-03-06 10:52:39                            12.590029   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            12.849593   \n",
       "2019-03-06 14:47:09                            12.852317   \n",
       "2019-03-06 14:47:14                            12.850616   \n",
       "2019-03-06 14:47:15                            12.852301   \n",
       "2019-03-06 14:47:19                            12.847159   \n",
       "\n",
       "                     Stage1.Output.Measurement2.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            11.108870   \n",
       "2019-03-06 10:52:35                            10.796167   \n",
       "2019-03-06 10:52:36                            10.807688   \n",
       "2019-03-06 10:52:37                            10.957961   \n",
       "2019-03-06 10:52:39                            10.428984   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             1.146642   \n",
       "2019-03-06 14:47:09                             1.153117   \n",
       "2019-03-06 14:47:14                             1.103623   \n",
       "2019-03-06 14:47:15                             1.134105   \n",
       "2019-03-06 14:47:19                             1.051081   \n",
       "\n",
       "                     Stage1.Output.Measurement3.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            11.525728   \n",
       "2019-03-06 10:52:35                            11.518107   \n",
       "2019-03-06 10:52:36                            11.494731   \n",
       "2019-03-06 10:52:37                            11.514869   \n",
       "2019-03-06 10:52:39                            11.209401   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            11.325775   \n",
       "2019-03-06 14:47:09                            11.325607   \n",
       "2019-03-06 14:47:14                            11.322406   \n",
       "2019-03-06 14:47:15                            11.324818   \n",
       "2019-03-06 14:47:19                            11.318814   \n",
       "\n",
       "                     Stage1.Output.Measurement4.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            21.446243   \n",
       "2019-03-06 10:52:35                            21.392389   \n",
       "2019-03-06 10:52:36                            21.369123   \n",
       "2019-03-06 10:52:37                            21.413769   \n",
       "2019-03-06 10:52:39                            20.901497   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            21.487192   \n",
       "2019-03-06 14:47:09                            21.496141   \n",
       "2019-03-06 14:47:14                            21.497688   \n",
       "2019-03-06 14:47:15                            21.499315   \n",
       "2019-03-06 14:47:19                            21.495878   \n",
       "\n",
       "                     Stage1.Output.Measurement5.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            31.974375   \n",
       "2019-03-06 10:52:35                            31.814201   \n",
       "2019-03-06 10:52:36                            31.795662   \n",
       "2019-03-06 10:52:37                            31.891699   \n",
       "2019-03-06 10:52:39                            30.987888   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            33.346756   \n",
       "2019-03-06 14:47:09                            33.379215   \n",
       "2019-03-06 14:47:14                            33.391533   \n",
       "2019-03-06 14:47:15                            33.389832   \n",
       "2019-03-06 14:47:19                            33.386353   \n",
       "\n",
       "                     Stage1.Output.Measurement6.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             0.100744   \n",
       "2019-03-06 10:52:35                             0.100739   \n",
       "2019-03-06 10:52:36                             0.100813   \n",
       "2019-03-06 10:52:37                             0.101447   \n",
       "2019-03-06 10:52:39                             0.088306   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             0.049396   \n",
       "2019-03-06 14:47:09                             0.048376   \n",
       "2019-03-06 14:47:14                             0.046241   \n",
       "2019-03-06 14:47:15                             0.047170   \n",
       "2019-03-06 14:47:19                             0.043773   \n",
       "\n",
       "                     Stage1.Output.Measurement7.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             2.394584   \n",
       "2019-03-06 10:52:35                             2.360145   \n",
       "2019-03-06 10:52:36                             2.359014   \n",
       "2019-03-06 10:52:37                             2.372782   \n",
       "2019-03-06 10:52:39                             2.202711   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             0.332409   \n",
       "2019-03-06 14:47:09                             0.318794   \n",
       "2019-03-06 14:47:14                             0.308263   \n",
       "2019-03-06 14:47:15                             0.311799   \n",
       "2019-03-06 14:47:19                             0.301311   \n",
       "\n",
       "                     Stage1.Output.Measurement8.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             2.968608   \n",
       "2019-03-06 10:52:35                             2.881801   \n",
       "2019-03-06 10:52:36                             2.890857   \n",
       "2019-03-06 10:52:37                             2.920685   \n",
       "2019-03-06 10:52:39                             2.635536   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            -0.012815   \n",
       "2019-03-06 14:47:09                            -0.025988   \n",
       "2019-03-06 14:47:14                            -0.021213   \n",
       "2019-03-06 14:47:15                            -0.024984   \n",
       "2019-03-06 14:47:19                            -0.006206   \n",
       "\n",
       "                     Stage1.Output.Measurement9.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            21.012835   \n",
       "2019-03-06 10:52:35                            20.888096   \n",
       "2019-03-06 10:52:36                            20.840380   \n",
       "2019-03-06 10:52:37                            20.953377   \n",
       "2019-03-06 10:52:39                            19.195074   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            20.700445   \n",
       "2019-03-06 14:47:09                            20.714857   \n",
       "2019-03-06 14:47:14                            20.722038   \n",
       "2019-03-06 14:47:15                            20.720697   \n",
       "2019-03-06 14:47:19                            20.722193   \n",
       "\n",
       "                     Stage1.Output.Measurement10.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                             18.983297   \n",
       "2019-03-06 10:52:35                             18.906410   \n",
       "2019-03-06 10:52:36                             18.861372   \n",
       "2019-03-06 10:52:37                             18.948387   \n",
       "2019-03-06 10:52:39                             17.458477   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                             18.849937   \n",
       "2019-03-06 14:47:09                             18.860575   \n",
       "2019-03-06 14:47:14                             18.868387   \n",
       "2019-03-06 14:47:15                             18.865589   \n",
       "2019-03-06 14:47:19                             18.871586   \n",
       "\n",
       "                     Stage1.Output.Measurement11.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              7.777860   \n",
       "2019-03-06 10:52:35                              7.756851   \n",
       "2019-03-06 10:52:36                              7.753476   \n",
       "2019-03-06 10:52:37                              7.767488   \n",
       "2019-03-06 10:52:39                              7.608414   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              7.647273   \n",
       "2019-03-06 14:47:09                              7.652046   \n",
       "2019-03-06 14:47:14                              7.652266   \n",
       "2019-03-06 14:47:15                              7.652730   \n",
       "2019-03-06 14:47:19                              7.650171   \n",
       "\n",
       "                     Stage1.Output.Measurement12.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              4.820940   \n",
       "2019-03-06 10:52:35                              4.728960   \n",
       "2019-03-06 10:52:36                              4.690063   \n",
       "2019-03-06 10:52:37                              4.739079   \n",
       "2019-03-06 10:52:39                              4.330048   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              0.140641   \n",
       "2019-03-06 14:47:09                              0.118026   \n",
       "2019-03-06 14:47:14                              0.108305   \n",
       "2019-03-06 14:47:15                              0.111721   \n",
       "2019-03-06 14:47:19                              0.109741   \n",
       "\n",
       "                     Stage1.Output.Measurement13.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              0.772798   \n",
       "2019-03-06 10:52:35                              0.779288   \n",
       "2019-03-06 10:52:36                              0.785036   \n",
       "2019-03-06 10:52:37                              0.782860   \n",
       "2019-03-06 10:52:39                              0.759154   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              1.413460   \n",
       "2019-03-06 14:47:09                              1.415275   \n",
       "2019-03-06 14:47:14                              1.415003   \n",
       "2019-03-06 14:47:15                              1.415589   \n",
       "2019-03-06 14:47:19                              1.413550   \n",
       "\n",
       "                     Stage1.Output.Measurement14.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              2.772620   \n",
       "2019-03-06 10:52:35                              2.769287   \n",
       "2019-03-06 10:52:36                              2.769865   \n",
       "2019-03-06 10:52:37                              2.774833   \n",
       "2019-03-06 10:52:39                              2.627956   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              3.295619   \n",
       "2019-03-06 14:47:09                              3.301226   \n",
       "2019-03-06 14:47:14                              3.306916   \n",
       "2019-03-06 14:47:15                              3.303921   \n",
       "2019-03-06 14:47:19                              3.310215   \n",
       "\n",
       "                     Machine4.Temperature1.C.Actual  \n",
       "time_stamp                                           \n",
       "2019-03-06 10:52:34                       14.534842  \n",
       "2019-03-06 10:52:35                       14.227365  \n",
       "2019-03-06 10:52:36                       14.270016  \n",
       "2019-03-06 10:52:37                       14.384173  \n",
       "2019-03-06 10:52:39                       13.159143  \n",
       "...                                             ...  \n",
       "2019-03-06 14:46:57                        2.207462  \n",
       "2019-03-06 14:47:09                        2.097956  \n",
       "2019-03-06 14:47:14                        2.002542  \n",
       "2019-03-06 14:47:15                        2.039164  \n",
       "2019-03-06 14:47:19                        1.928090  \n",
       "\n",
       "[2818 rows x 15 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage2 x데이터 출력\n",
    "X_stage2 = pd.concat([raw_df_machine0,raw_df_machine4,raw_df_machine5],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage2 y데이터 출력\n",
    "y_stage2 = raw_df_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage2 데이터 전처리\n",
    "#특정컬럼제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage2 데이터 전처리\n",
    "df_scaled_stage2 = scaler.fit_transform(X_stage2)\n",
    "df_sclaed_df_stage2 = pd.DataFrame(df_scaled_stage2)\n",
    "X_stage2_scaler = df_sclaed_df_stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage2 훈련 정답데이터 분리\n",
    "X_train_stage2,X_test_stage2,y_train_stage2,y_test_stage2 = train_test_split(X_stage2_scaler,y_stage2,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 20)                1760      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               2688      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 15)                75        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15455 (60.37 KB)\n",
      "Trainable params: 15455 (60.37 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#stage2 모델만들기\n",
    "model_stage2 = Sequential()\n",
    "model_stage2.add(LSTM(units=20,input_shape=(16,1)))\n",
    "model_stage2.add(Dense(units = 128,activation='relu')) #활성화함수를 안에 같이 써줘도 되고 따로 써줘도 무방함.\n",
    "model_stage2.add(Dense(units = 64,activation='relu'))\n",
    "model_stage2.add(Dense(units = 32,activation='relu'))\n",
    "model_stage2.add(Dense(units = 16,activation='relu'))\n",
    "model_stage2.add(Dense(units = 4,activation='relu'))\n",
    "model_stage2.add(tf.keras.layers.Dropout(0.2)),\n",
    "model_stage2.add(Dense(units=15))\n",
    "\n",
    "model_stage2.summary()\n",
    "model_stage2.compile(loss='mean_squared_error',optimizer='adam',metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "226/226 [==============================] - 4s 7ms/step - loss: 56.9165 - mse: 56.9165 - val_loss: 23.3109 - val_mse: 23.3109\n",
      "Epoch 2/300\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 29.0219 - mse: 29.0219 - val_loss: 16.0860 - val_mse: 16.0860\n",
      "Epoch 3/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 24.5891 - mse: 24.5891 - val_loss: 14.0332 - val_mse: 14.0332\n",
      "Epoch 4/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 23.4192 - mse: 23.4192 - val_loss: 14.3864 - val_mse: 14.3864\n",
      "Epoch 5/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 22.3343 - mse: 22.3343 - val_loss: 13.6064 - val_mse: 13.6064\n",
      "Epoch 6/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 21.8779 - mse: 21.8779 - val_loss: 13.9026 - val_mse: 13.9026\n",
      "Epoch 7/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 22.0477 - mse: 22.0477 - val_loss: 14.0427 - val_mse: 14.0427\n",
      "Epoch 8/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 20.9693 - mse: 20.9693 - val_loss: 13.5044 - val_mse: 13.5044\n",
      "Epoch 9/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 21.0284 - mse: 21.0284 - val_loss: 13.5255 - val_mse: 13.5255\n",
      "Epoch 10/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 20.7081 - mse: 20.7081 - val_loss: 13.7257 - val_mse: 13.7257\n",
      "Epoch 11/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 20.5468 - mse: 20.5468 - val_loss: 13.1751 - val_mse: 13.1751\n",
      "Epoch 12/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 20.3382 - mse: 20.3382 - val_loss: 13.0870 - val_mse: 13.0870\n",
      "Epoch 13/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 20.0299 - mse: 20.0299 - val_loss: 14.1613 - val_mse: 14.1613\n",
      "Epoch 14/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 19.4687 - mse: 19.4687 - val_loss: 13.4480 - val_mse: 13.4480\n",
      "Epoch 15/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 19.5926 - mse: 19.5926 - val_loss: 12.9455 - val_mse: 12.9455\n",
      "Epoch 16/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 19.7602 - mse: 19.7602 - val_loss: 12.8043 - val_mse: 12.8043\n",
      "Epoch 17/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 19.1661 - mse: 19.1661 - val_loss: 13.2897 - val_mse: 13.2897\n",
      "Epoch 18/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 19.0925 - mse: 19.0925 - val_loss: 13.1730 - val_mse: 13.1730\n",
      "Epoch 19/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 18.9511 - mse: 18.9511 - val_loss: 14.0299 - val_mse: 14.0299\n",
      "Epoch 20/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 18.6709 - mse: 18.6709 - val_loss: 13.3075 - val_mse: 13.3075\n",
      "Epoch 21/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 18.2495 - mse: 18.2495 - val_loss: 13.4083 - val_mse: 13.4083\n",
      "Epoch 22/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 18.3942 - mse: 18.3942 - val_loss: 13.6592 - val_mse: 13.6592\n",
      "Epoch 23/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 18.2648 - mse: 18.2648 - val_loss: 13.6861 - val_mse: 13.6861\n",
      "Epoch 24/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 17.9225 - mse: 17.9225 - val_loss: 13.8447 - val_mse: 13.8447\n",
      "Epoch 25/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 17.6949 - mse: 17.6949 - val_loss: 12.5635 - val_mse: 12.5635\n",
      "Epoch 26/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 17.2183 - mse: 17.2183 - val_loss: 12.6175 - val_mse: 12.6175\n",
      "Epoch 27/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 17.0067 - mse: 17.0067 - val_loss: 13.3056 - val_mse: 13.3056\n",
      "Epoch 28/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 16.9587 - mse: 16.9587 - val_loss: 12.2815 - val_mse: 12.2815\n",
      "Epoch 29/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 16.8624 - mse: 16.8624 - val_loss: 12.5580 - val_mse: 12.5580\n",
      "Epoch 30/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 16.8600 - mse: 16.8600 - val_loss: 12.6147 - val_mse: 12.6147\n",
      "Epoch 31/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 16.5240 - mse: 16.5240 - val_loss: 12.5690 - val_mse: 12.5690\n",
      "Epoch 32/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 16.2686 - mse: 16.2686 - val_loss: 12.3467 - val_mse: 12.3467\n",
      "Epoch 33/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 16.0741 - mse: 16.0741 - val_loss: 12.4185 - val_mse: 12.4185\n",
      "Epoch 34/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.9606 - mse: 15.9606 - val_loss: 12.8524 - val_mse: 12.8524\n",
      "Epoch 35/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.8827 - mse: 15.8827 - val_loss: 12.0281 - val_mse: 12.0281\n",
      "Epoch 36/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.5792 - mse: 15.5792 - val_loss: 12.2029 - val_mse: 12.2029\n",
      "Epoch 37/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.4620 - mse: 15.4620 - val_loss: 12.5814 - val_mse: 12.5814\n",
      "Epoch 38/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.5473 - mse: 15.5473 - val_loss: 12.3051 - val_mse: 12.3051\n",
      "Epoch 39/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.3882 - mse: 15.3882 - val_loss: 12.3620 - val_mse: 12.3620\n",
      "Epoch 40/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.3852 - mse: 15.3852 - val_loss: 12.3705 - val_mse: 12.3705\n",
      "Epoch 41/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.0983 - mse: 15.0983 - val_loss: 12.5853 - val_mse: 12.5853\n",
      "Epoch 42/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 15.0773 - mse: 15.0773 - val_loss: 12.0021 - val_mse: 12.0021\n",
      "Epoch 43/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.8383 - mse: 14.8383 - val_loss: 12.0740 - val_mse: 12.0740\n",
      "Epoch 44/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 14.6453 - mse: 14.6453 - val_loss: 12.1540 - val_mse: 12.1540\n",
      "Epoch 45/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.6325 - mse: 14.6325 - val_loss: 12.2316 - val_mse: 12.2316\n",
      "Epoch 46/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.5587 - mse: 14.5587 - val_loss: 12.5406 - val_mse: 12.5406\n",
      "Epoch 47/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.4801 - mse: 14.4801 - val_loss: 12.0655 - val_mse: 12.0655\n",
      "Epoch 48/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.2653 - mse: 14.2653 - val_loss: 11.7998 - val_mse: 11.7998\n",
      "Epoch 49/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.2354 - mse: 14.2354 - val_loss: 12.3264 - val_mse: 12.3264\n",
      "Epoch 50/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.2439 - mse: 14.2439 - val_loss: 12.0003 - val_mse: 12.0003\n",
      "Epoch 51/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 14.0321 - mse: 14.0321 - val_loss: 11.8852 - val_mse: 11.8852\n",
      "Epoch 52/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 13.8910 - mse: 13.8910 - val_loss: 12.4179 - val_mse: 12.4179\n",
      "Epoch 53/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 13.8924 - mse: 13.8924 - val_loss: 12.2203 - val_mse: 12.2203\n",
      "Epoch 54/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 13.7523 - mse: 13.7523 - val_loss: 11.8594 - val_mse: 11.8594\n",
      "Epoch 55/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 13.6219 - mse: 13.6219 - val_loss: 12.3515 - val_mse: 12.3515\n",
      "Epoch 56/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 13.4195 - mse: 13.4195 - val_loss: 11.5740 - val_mse: 11.5740\n",
      "Epoch 57/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 13.1094 - mse: 13.1094 - val_loss: 10.5519 - val_mse: 10.5519\n",
      "Epoch 58/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.9082 - mse: 12.9082 - val_loss: 10.4587 - val_mse: 10.4587\n",
      "Epoch 59/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.7361 - mse: 12.7361 - val_loss: 10.4428 - val_mse: 10.4428\n",
      "Epoch 60/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.5070 - mse: 12.5070 - val_loss: 10.3070 - val_mse: 10.3070\n",
      "Epoch 61/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.5029 - mse: 12.5029 - val_loss: 10.0740 - val_mse: 10.0740\n",
      "Epoch 62/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.3025 - mse: 12.3025 - val_loss: 9.8282 - val_mse: 9.8282\n",
      "Epoch 63/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.1187 - mse: 12.1187 - val_loss: 10.0649 - val_mse: 10.0649\n",
      "Epoch 64/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 12.2373 - mse: 12.2373 - val_loss: 10.3186 - val_mse: 10.3186\n",
      "Epoch 65/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 12.0106 - mse: 12.0106 - val_loss: 9.8250 - val_mse: 9.8250\n",
      "Epoch 66/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.9927 - mse: 11.9927 - val_loss: 9.9308 - val_mse: 9.9308\n",
      "Epoch 67/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.9462 - mse: 11.9462 - val_loss: 9.6141 - val_mse: 9.6141\n",
      "Epoch 68/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.6367 - mse: 11.6367 - val_loss: 9.4170 - val_mse: 9.4170\n",
      "Epoch 69/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.5212 - mse: 11.5212 - val_loss: 9.4507 - val_mse: 9.4507\n",
      "Epoch 70/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.3898 - mse: 11.3898 - val_loss: 9.5518 - val_mse: 9.5518\n",
      "Epoch 71/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.4798 - mse: 11.4798 - val_loss: 9.2122 - val_mse: 9.2122\n",
      "Epoch 72/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.2991 - mse: 11.2991 - val_loss: 9.9217 - val_mse: 9.9217\n",
      "Epoch 73/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.2450 - mse: 11.2450 - val_loss: 9.3577 - val_mse: 9.3577\n",
      "Epoch 74/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.0716 - mse: 11.0716 - val_loss: 9.0607 - val_mse: 9.0607\n",
      "Epoch 75/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 11.1320 - mse: 11.1320 - val_loss: 9.0123 - val_mse: 9.0123\n",
      "Epoch 76/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.9951 - mse: 10.9951 - val_loss: 8.9651 - val_mse: 8.9651\n",
      "Epoch 77/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.9321 - mse: 10.9321 - val_loss: 9.0616 - val_mse: 9.0616\n",
      "Epoch 78/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.7180 - mse: 10.7180 - val_loss: 9.4363 - val_mse: 9.4363\n",
      "Epoch 79/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.8469 - mse: 10.8469 - val_loss: 8.7502 - val_mse: 8.7502\n",
      "Epoch 80/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.5602 - mse: 10.5602 - val_loss: 8.9224 - val_mse: 8.9224\n",
      "Epoch 81/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.5092 - mse: 10.5092 - val_loss: 9.3099 - val_mse: 9.3099\n",
      "Epoch 82/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 10.4414 - mse: 10.4414 - val_loss: 8.5670 - val_mse: 8.5670\n",
      "Epoch 83/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.4010 - mse: 10.4010 - val_loss: 8.8624 - val_mse: 8.8624\n",
      "Epoch 84/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.3940 - mse: 10.3940 - val_loss: 8.6936 - val_mse: 8.6936\n",
      "Epoch 85/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.1846 - mse: 10.1846 - val_loss: 8.8464 - val_mse: 8.8464\n",
      "Epoch 86/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.1695 - mse: 10.1695 - val_loss: 8.3572 - val_mse: 8.3572\n",
      "Epoch 87/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.0554 - mse: 10.0554 - val_loss: 8.5909 - val_mse: 8.5909\n",
      "Epoch 88/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 10.0873 - mse: 10.0873 - val_loss: 8.1590 - val_mse: 8.1590\n",
      "Epoch 89/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 9.4099 - mse: 9.4099 - val_loss: 7.2177 - val_mse: 7.2177\n",
      "Epoch 90/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 9.2092 - mse: 9.2092 - val_loss: 7.9420 - val_mse: 7.9420\n",
      "Epoch 91/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 9.1300 - mse: 9.1300 - val_loss: 7.5298 - val_mse: 7.5298\n",
      "Epoch 92/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 9.0173 - mse: 9.0173 - val_loss: 7.4541 - val_mse: 7.4541\n",
      "Epoch 93/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.8209 - mse: 8.8209 - val_loss: 7.2537 - val_mse: 7.2537\n",
      "Epoch 94/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.8800 - mse: 8.8800 - val_loss: 7.2257 - val_mse: 7.2257\n",
      "Epoch 95/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.8131 - mse: 8.8131 - val_loss: 7.5533 - val_mse: 7.5533\n",
      "Epoch 96/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.7636 - mse: 8.7636 - val_loss: 7.1574 - val_mse: 7.1574\n",
      "Epoch 97/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.7673 - mse: 8.7673 - val_loss: 7.6535 - val_mse: 7.6535\n",
      "Epoch 98/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.8565 - mse: 8.8565 - val_loss: 7.4198 - val_mse: 7.4198\n",
      "Epoch 99/300\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 8.7763 - mse: 8.7763 - val_loss: 7.2064 - val_mse: 7.2064\n",
      "Epoch 100/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.7363 - mse: 8.7363 - val_loss: 7.3127 - val_mse: 7.3127\n",
      "Epoch 101/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.8223 - mse: 8.8223 - val_loss: 7.2263 - val_mse: 7.2263\n",
      "Epoch 102/300\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 8.7559 - mse: 8.7559 - val_loss: 7.3192 - val_mse: 7.3192\n",
      "Epoch 103/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.6982 - mse: 8.6982 - val_loss: 7.0057 - val_mse: 7.0057\n",
      "Epoch 104/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.5005 - mse: 8.5005 - val_loss: 7.4723 - val_mse: 7.4723\n",
      "Epoch 105/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.7328 - mse: 8.7328 - val_loss: 7.1681 - val_mse: 7.1681\n",
      "Epoch 106/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.6647 - mse: 8.6647 - val_loss: 7.0609 - val_mse: 7.0609\n",
      "Epoch 107/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.4199 - mse: 8.4199 - val_loss: 7.0964 - val_mse: 7.0964\n",
      "Epoch 108/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.5918 - mse: 8.5918 - val_loss: 7.0602 - val_mse: 7.0602\n",
      "Epoch 109/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.5660 - mse: 8.5660 - val_loss: 7.1643 - val_mse: 7.1643\n",
      "Epoch 110/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.4585 - mse: 8.4585 - val_loss: 7.0371 - val_mse: 7.0371\n",
      "Epoch 111/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.6239 - mse: 8.6239 - val_loss: 7.1048 - val_mse: 7.1048\n",
      "Epoch 112/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2662 - mse: 8.2662 - val_loss: 7.1330 - val_mse: 7.1330\n",
      "Epoch 113/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.4452 - mse: 8.4452 - val_loss: 6.8911 - val_mse: 6.8911\n",
      "Epoch 114/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.4278 - mse: 8.4278 - val_loss: 7.1870 - val_mse: 7.1870\n",
      "Epoch 115/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.3779 - mse: 8.3779 - val_loss: 7.2288 - val_mse: 7.2288\n",
      "Epoch 116/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.5200 - mse: 8.5200 - val_loss: 6.8859 - val_mse: 6.8859\n",
      "Epoch 117/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.3723 - mse: 8.3723 - val_loss: 7.6964 - val_mse: 7.6964\n",
      "Epoch 118/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2754 - mse: 8.2754 - val_loss: 6.7822 - val_mse: 6.7822\n",
      "Epoch 119/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.5155 - mse: 8.5155 - val_loss: 7.0326 - val_mse: 7.0326\n",
      "Epoch 120/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2911 - mse: 8.2911 - val_loss: 7.0195 - val_mse: 7.0195\n",
      "Epoch 121/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.4394 - mse: 8.4394 - val_loss: 7.2217 - val_mse: 7.2217\n",
      "Epoch 122/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.1935 - mse: 8.1935 - val_loss: 6.8921 - val_mse: 6.8921\n",
      "Epoch 123/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.4227 - mse: 8.4227 - val_loss: 6.7596 - val_mse: 6.7596\n",
      "Epoch 124/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.0890 - mse: 8.0890 - val_loss: 7.5281 - val_mse: 7.5281\n",
      "Epoch 125/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.1298 - mse: 8.1298 - val_loss: 7.2129 - val_mse: 7.2129\n",
      "Epoch 126/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.1798 - mse: 8.1798 - val_loss: 7.1071 - val_mse: 7.1071\n",
      "Epoch 127/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.1053 - mse: 8.1053 - val_loss: 6.7725 - val_mse: 6.7725\n",
      "Epoch 128/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.1327 - mse: 8.1327 - val_loss: 6.8891 - val_mse: 6.8891\n",
      "Epoch 129/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.1963 - mse: 8.1963 - val_loss: 7.1993 - val_mse: 7.1993\n",
      "Epoch 130/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9340 - mse: 7.9340 - val_loss: 6.9490 - val_mse: 6.9490\n",
      "Epoch 131/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2463 - mse: 8.2463 - val_loss: 7.0405 - val_mse: 7.0405\n",
      "Epoch 132/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2946 - mse: 8.2946 - val_loss: 6.5491 - val_mse: 6.5491\n",
      "Epoch 133/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2045 - mse: 8.2045 - val_loss: 6.6120 - val_mse: 6.6120\n",
      "Epoch 134/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.1276 - mse: 8.1276 - val_loss: 6.7105 - val_mse: 6.7105\n",
      "Epoch 135/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9863 - mse: 7.9863 - val_loss: 6.7218 - val_mse: 6.7218\n",
      "Epoch 136/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.1212 - mse: 8.1212 - val_loss: 6.6305 - val_mse: 6.6305\n",
      "Epoch 137/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.1881 - mse: 8.1881 - val_loss: 6.8086 - val_mse: 6.8086\n",
      "Epoch 138/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.2196 - mse: 8.2196 - val_loss: 6.6403 - val_mse: 6.6403\n",
      "Epoch 139/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.1249 - mse: 8.1249 - val_loss: 6.8230 - val_mse: 6.8230\n",
      "Epoch 140/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.0688 - mse: 8.0688 - val_loss: 6.5801 - val_mse: 6.5801\n",
      "Epoch 141/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9554 - mse: 7.9554 - val_loss: 6.6694 - val_mse: 6.6694\n",
      "Epoch 142/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.0407 - mse: 8.0407 - val_loss: 6.7924 - val_mse: 6.7924\n",
      "Epoch 143/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.2936 - mse: 8.2936 - val_loss: 7.2865 - val_mse: 7.2865\n",
      "Epoch 144/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.0670 - mse: 8.0670 - val_loss: 6.8360 - val_mse: 6.8360\n",
      "Epoch 145/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8013 - mse: 7.8013 - val_loss: 6.5836 - val_mse: 6.5836\n",
      "Epoch 146/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 8.0383 - mse: 8.0383 - val_loss: 7.0725 - val_mse: 7.0725\n",
      "Epoch 147/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7899 - mse: 7.7899 - val_loss: 6.7105 - val_mse: 6.7105\n",
      "Epoch 148/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.4686 - mse: 8.4686 - val_loss: 6.7953 - val_mse: 6.7953\n",
      "Epoch 149/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.9401 - mse: 7.9401 - val_loss: 6.6999 - val_mse: 6.6999\n",
      "Epoch 150/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8968 - mse: 7.8968 - val_loss: 6.5266 - val_mse: 6.5266\n",
      "Epoch 151/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.8370 - mse: 7.8370 - val_loss: 7.1211 - val_mse: 7.1211\n",
      "Epoch 152/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 8.0875 - mse: 8.0875 - val_loss: 6.6657 - val_mse: 6.6657\n",
      "Epoch 153/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.9811 - mse: 7.9811 - val_loss: 6.3877 - val_mse: 6.3877\n",
      "Epoch 154/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.9372 - mse: 7.9372 - val_loss: 6.6613 - val_mse: 6.6613\n",
      "Epoch 155/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.8526 - mse: 7.8526 - val_loss: 6.8922 - val_mse: 6.8922\n",
      "Epoch 156/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7927 - mse: 7.7927 - val_loss: 6.6965 - val_mse: 6.6965\n",
      "Epoch 157/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9277 - mse: 7.9277 - val_loss: 7.3070 - val_mse: 7.3070\n",
      "Epoch 158/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7946 - mse: 7.7946 - val_loss: 6.4462 - val_mse: 6.4462\n",
      "Epoch 159/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8284 - mse: 7.8284 - val_loss: 6.6696 - val_mse: 6.6696\n",
      "Epoch 160/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8996 - mse: 7.8996 - val_loss: 6.2808 - val_mse: 6.2808\n",
      "Epoch 161/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9501 - mse: 7.9501 - val_loss: 7.4071 - val_mse: 7.4071\n",
      "Epoch 162/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.8688 - mse: 7.8688 - val_loss: 6.5571 - val_mse: 6.5571\n",
      "Epoch 163/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9540 - mse: 7.9540 - val_loss: 6.2779 - val_mse: 6.2779\n",
      "Epoch 164/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7741 - mse: 7.7741 - val_loss: 6.6065 - val_mse: 6.6065\n",
      "Epoch 165/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.8431 - mse: 7.8431 - val_loss: 6.7664 - val_mse: 6.7664\n",
      "Epoch 166/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.6612 - mse: 7.6612 - val_loss: 6.2810 - val_mse: 6.2810\n",
      "Epoch 167/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8298 - mse: 7.8298 - val_loss: 6.5243 - val_mse: 6.5243\n",
      "Epoch 168/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.9981 - mse: 7.9981 - val_loss: 6.4724 - val_mse: 6.4724\n",
      "Epoch 169/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7145 - mse: 7.7145 - val_loss: 6.4134 - val_mse: 6.4134\n",
      "Epoch 170/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6439 - mse: 7.6439 - val_loss: 6.2793 - val_mse: 6.2793\n",
      "Epoch 171/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6440 - mse: 7.6440 - val_loss: 6.1038 - val_mse: 6.1038\n",
      "Epoch 172/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6607 - mse: 7.6607 - val_loss: 6.2750 - val_mse: 6.2750\n",
      "Epoch 173/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6912 - mse: 7.6912 - val_loss: 6.5439 - val_mse: 6.5439\n",
      "Epoch 174/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8142 - mse: 7.8142 - val_loss: 6.8094 - val_mse: 6.8094\n",
      "Epoch 175/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7239 - mse: 7.7239 - val_loss: 6.6301 - val_mse: 6.6301\n",
      "Epoch 176/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7282 - mse: 7.7282 - val_loss: 6.3618 - val_mse: 6.3618\n",
      "Epoch 177/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5265 - mse: 7.5265 - val_loss: 6.3049 - val_mse: 6.3049\n",
      "Epoch 178/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.7002 - mse: 7.7002 - val_loss: 7.0416 - val_mse: 7.0416\n",
      "Epoch 179/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8000 - mse: 7.8000 - val_loss: 7.0277 - val_mse: 7.0277\n",
      "Epoch 180/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.7267 - mse: 7.7267 - val_loss: 6.5253 - val_mse: 6.5253\n",
      "Epoch 181/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5587 - mse: 7.5587 - val_loss: 6.3519 - val_mse: 6.3519\n",
      "Epoch 182/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7699 - mse: 7.7699 - val_loss: 6.0424 - val_mse: 6.0424\n",
      "Epoch 183/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5810 - mse: 7.5810 - val_loss: 6.2854 - val_mse: 6.2854\n",
      "Epoch 184/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8203 - mse: 7.8203 - val_loss: 6.4866 - val_mse: 6.4866\n",
      "Epoch 185/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.8036 - mse: 7.8036 - val_loss: 6.2711 - val_mse: 6.2711\n",
      "Epoch 186/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5954 - mse: 7.5954 - val_loss: 6.4650 - val_mse: 6.4650\n",
      "Epoch 187/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6262 - mse: 7.6262 - val_loss: 6.3826 - val_mse: 6.3826\n",
      "Epoch 188/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6266 - mse: 7.6266 - val_loss: 6.1213 - val_mse: 6.1213\n",
      "Epoch 189/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6084 - mse: 7.6084 - val_loss: 6.3181 - val_mse: 6.3181\n",
      "Epoch 190/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6158 - mse: 7.6158 - val_loss: 6.0839 - val_mse: 6.0839\n",
      "Epoch 191/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.7617 - mse: 7.7617 - val_loss: 6.1816 - val_mse: 6.1816\n",
      "Epoch 192/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.7047 - mse: 7.7047 - val_loss: 6.2042 - val_mse: 6.2042\n",
      "Epoch 193/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5736 - mse: 7.5736 - val_loss: 6.1954 - val_mse: 6.1954\n",
      "Epoch 194/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5929 - mse: 7.5929 - val_loss: 6.2847 - val_mse: 6.2847\n",
      "Epoch 195/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5829 - mse: 7.5829 - val_loss: 6.2645 - val_mse: 6.2645\n",
      "Epoch 196/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5534 - mse: 7.5534 - val_loss: 6.4183 - val_mse: 6.4183\n",
      "Epoch 197/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4828 - mse: 7.4828 - val_loss: 6.1708 - val_mse: 6.1708\n",
      "Epoch 198/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2726 - mse: 7.2726 - val_loss: 6.3847 - val_mse: 6.3847\n",
      "Epoch 199/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6044 - mse: 7.6044 - val_loss: 6.2200 - val_mse: 6.2200\n",
      "Epoch 200/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5441 - mse: 7.5441 - val_loss: 6.3712 - val_mse: 6.3712\n",
      "Epoch 201/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5711 - mse: 7.5711 - val_loss: 6.3035 - val_mse: 6.3035\n",
      "Epoch 202/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3459 - mse: 7.3459 - val_loss: 6.6279 - val_mse: 6.6279\n",
      "Epoch 203/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4840 - mse: 7.4840 - val_loss: 6.2240 - val_mse: 6.2240\n",
      "Epoch 204/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5915 - mse: 7.5915 - val_loss: 6.4408 - val_mse: 6.4408\n",
      "Epoch 205/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5001 - mse: 7.5001 - val_loss: 6.0762 - val_mse: 6.0762\n",
      "Epoch 206/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.6499 - mse: 7.6499 - val_loss: 6.3145 - val_mse: 6.3145\n",
      "Epoch 207/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.6268 - mse: 7.6268 - val_loss: 5.9730 - val_mse: 5.9730\n",
      "Epoch 208/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5622 - mse: 7.5622 - val_loss: 6.5970 - val_mse: 6.5970\n",
      "Epoch 209/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3825 - mse: 7.3825 - val_loss: 6.3991 - val_mse: 6.3991\n",
      "Epoch 210/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3724 - mse: 7.3724 - val_loss: 6.2470 - val_mse: 6.2470\n",
      "Epoch 211/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4382 - mse: 7.4382 - val_loss: 5.9021 - val_mse: 5.9021\n",
      "Epoch 212/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4934 - mse: 7.4934 - val_loss: 5.9437 - val_mse: 5.9437\n",
      "Epoch 213/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4206 - mse: 7.4206 - val_loss: 6.3307 - val_mse: 6.3307\n",
      "Epoch 214/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2144 - mse: 7.2144 - val_loss: 6.2773 - val_mse: 6.2773\n",
      "Epoch 215/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4223 - mse: 7.4223 - val_loss: 6.1279 - val_mse: 6.1279\n",
      "Epoch 216/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7266 - mse: 7.7266 - val_loss: 6.1891 - val_mse: 6.1891\n",
      "Epoch 217/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5408 - mse: 7.5408 - val_loss: 6.1608 - val_mse: 6.1608\n",
      "Epoch 218/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5718 - mse: 7.5718 - val_loss: 6.1304 - val_mse: 6.1304\n",
      "Epoch 219/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4550 - mse: 7.4550 - val_loss: 5.9773 - val_mse: 5.9773\n",
      "Epoch 220/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5230 - mse: 7.5230 - val_loss: 5.9705 - val_mse: 5.9705\n",
      "Epoch 221/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.6163 - mse: 7.6163 - val_loss: 6.3071 - val_mse: 6.3071\n",
      "Epoch 222/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3547 - mse: 7.3547 - val_loss: 5.9304 - val_mse: 5.9304\n",
      "Epoch 223/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4744 - mse: 7.4744 - val_loss: 6.1379 - val_mse: 6.1379\n",
      "Epoch 224/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4038 - mse: 7.4038 - val_loss: 6.0003 - val_mse: 6.0003\n",
      "Epoch 225/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4810 - mse: 7.4810 - val_loss: 6.2709 - val_mse: 6.2709\n",
      "Epoch 226/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2125 - mse: 7.2125 - val_loss: 5.8407 - val_mse: 5.8407\n",
      "Epoch 227/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.7829 - mse: 7.7829 - val_loss: 5.8247 - val_mse: 5.8247\n",
      "Epoch 228/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1605 - mse: 7.1605 - val_loss: 5.8279 - val_mse: 5.8279\n",
      "Epoch 229/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3681 - mse: 7.3681 - val_loss: 6.0731 - val_mse: 6.0731\n",
      "Epoch 230/300\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 7.3228 - mse: 7.3228 - val_loss: 5.9425 - val_mse: 5.9425\n",
      "Epoch 231/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5131 - mse: 7.5131 - val_loss: 5.8164 - val_mse: 5.8164\n",
      "Epoch 232/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4683 - mse: 7.4683 - val_loss: 5.7959 - val_mse: 5.7959\n",
      "Epoch 233/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3038 - mse: 7.3038 - val_loss: 6.0529 - val_mse: 6.0529\n",
      "Epoch 234/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3962 - mse: 7.3962 - val_loss: 6.0628 - val_mse: 6.0628\n",
      "Epoch 235/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4351 - mse: 7.4351 - val_loss: 5.9405 - val_mse: 5.9405\n",
      "Epoch 236/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4243 - mse: 7.4243 - val_loss: 5.9719 - val_mse: 5.9719\n",
      "Epoch 237/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3221 - mse: 7.3221 - val_loss: 6.4678 - val_mse: 6.4678\n",
      "Epoch 238/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3126 - mse: 7.3126 - val_loss: 6.0273 - val_mse: 6.0273\n",
      "Epoch 239/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3297 - mse: 7.3297 - val_loss: 5.9608 - val_mse: 5.9608\n",
      "Epoch 240/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3698 - mse: 7.3698 - val_loss: 5.9905 - val_mse: 5.9905\n",
      "Epoch 241/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2842 - mse: 7.2842 - val_loss: 6.1036 - val_mse: 6.1036\n",
      "Epoch 242/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3649 - mse: 7.3649 - val_loss: 6.2308 - val_mse: 6.2308\n",
      "Epoch 243/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3597 - mse: 7.3597 - val_loss: 5.8171 - val_mse: 5.8171\n",
      "Epoch 244/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3321 - mse: 7.3321 - val_loss: 6.0685 - val_mse: 6.0685\n",
      "Epoch 245/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2985 - mse: 7.2985 - val_loss: 5.7352 - val_mse: 5.7352\n",
      "Epoch 246/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3075 - mse: 7.3075 - val_loss: 6.0865 - val_mse: 6.0865\n",
      "Epoch 247/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5606 - mse: 7.5606 - val_loss: 5.9311 - val_mse: 5.9311\n",
      "Epoch 248/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3307 - mse: 7.3307 - val_loss: 5.7603 - val_mse: 5.7603\n",
      "Epoch 249/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0622 - mse: 7.0622 - val_loss: 6.0252 - val_mse: 6.0252\n",
      "Epoch 250/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1995 - mse: 7.1995 - val_loss: 5.8066 - val_mse: 5.8066\n",
      "Epoch 251/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.4767 - mse: 7.4767 - val_loss: 5.8918 - val_mse: 5.8918\n",
      "Epoch 252/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2965 - mse: 7.2965 - val_loss: 5.9088 - val_mse: 5.9088\n",
      "Epoch 253/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1379 - mse: 7.1379 - val_loss: 6.2022 - val_mse: 6.2022\n",
      "Epoch 254/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3106 - mse: 7.3106 - val_loss: 5.7857 - val_mse: 5.7857\n",
      "Epoch 255/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1330 - mse: 7.1330 - val_loss: 6.1599 - val_mse: 6.1599\n",
      "Epoch 256/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2072 - mse: 7.2072 - val_loss: 5.8822 - val_mse: 5.8822\n",
      "Epoch 257/300\n",
      "226/226 [==============================] - 2s 7ms/step - loss: 7.3431 - mse: 7.3431 - val_loss: 6.0642 - val_mse: 6.0642\n",
      "Epoch 258/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3938 - mse: 7.3938 - val_loss: 6.2265 - val_mse: 6.2265\n",
      "Epoch 259/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4012 - mse: 7.4012 - val_loss: 5.8405 - val_mse: 5.8405\n",
      "Epoch 260/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2483 - mse: 7.2483 - val_loss: 5.8567 - val_mse: 5.8567\n",
      "Epoch 261/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2055 - mse: 7.2055 - val_loss: 6.0451 - val_mse: 6.0451\n",
      "Epoch 262/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.5071 - mse: 7.5071 - val_loss: 5.6119 - val_mse: 5.6119\n",
      "Epoch 263/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0795 - mse: 7.0795 - val_loss: 5.7600 - val_mse: 5.7600\n",
      "Epoch 264/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3217 - mse: 7.3217 - val_loss: 5.8363 - val_mse: 5.8363\n",
      "Epoch 265/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2064 - mse: 7.2064 - val_loss: 5.7051 - val_mse: 5.7051\n",
      "Epoch 266/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1577 - mse: 7.1577 - val_loss: 5.8186 - val_mse: 5.8186\n",
      "Epoch 267/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2120 - mse: 7.2120 - val_loss: 5.9627 - val_mse: 5.9627\n",
      "Epoch 268/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3581 - mse: 7.3581 - val_loss: 5.7939 - val_mse: 5.7939\n",
      "Epoch 269/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3590 - mse: 7.3590 - val_loss: 5.8544 - val_mse: 5.8544\n",
      "Epoch 270/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3435 - mse: 7.3435 - val_loss: 5.8575 - val_mse: 5.8575\n",
      "Epoch 271/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.1518 - mse: 7.1518 - val_loss: 5.9089 - val_mse: 5.9089\n",
      "Epoch 272/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2136 - mse: 7.2136 - val_loss: 5.7954 - val_mse: 5.7954\n",
      "Epoch 273/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3268 - mse: 7.3268 - val_loss: 5.5667 - val_mse: 5.5667\n",
      "Epoch 274/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1586 - mse: 7.1586 - val_loss: 6.1020 - val_mse: 6.1020\n",
      "Epoch 275/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3260 - mse: 7.3260 - val_loss: 5.9893 - val_mse: 5.9893\n",
      "Epoch 276/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3864 - mse: 7.3864 - val_loss: 5.5515 - val_mse: 5.5515\n",
      "Epoch 277/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2315 - mse: 7.2315 - val_loss: 5.8157 - val_mse: 5.8157\n",
      "Epoch 278/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0578 - mse: 7.0578 - val_loss: 5.7662 - val_mse: 5.7662\n",
      "Epoch 279/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1786 - mse: 7.1786 - val_loss: 5.6985 - val_mse: 5.6985\n",
      "Epoch 280/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.5365 - mse: 7.5365 - val_loss: 5.7012 - val_mse: 5.7012\n",
      "Epoch 281/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1087 - mse: 7.1087 - val_loss: 5.7607 - val_mse: 5.7607\n",
      "Epoch 282/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1122 - mse: 7.1122 - val_loss: 6.1593 - val_mse: 6.1593\n",
      "Epoch 283/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.3137 - mse: 7.3137 - val_loss: 5.7155 - val_mse: 5.7155\n",
      "Epoch 284/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2523 - mse: 7.2523 - val_loss: 5.7315 - val_mse: 5.7315\n",
      "Epoch 285/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.1109 - mse: 7.1109 - val_loss: 6.7503 - val_mse: 6.7503\n",
      "Epoch 286/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3173 - mse: 7.3173 - val_loss: 5.8629 - val_mse: 5.8629\n",
      "Epoch 287/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2646 - mse: 7.2646 - val_loss: 5.8983 - val_mse: 5.8983\n",
      "Epoch 288/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2678 - mse: 7.2678 - val_loss: 5.8941 - val_mse: 5.8941\n",
      "Epoch 289/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 6.9009 - mse: 6.9009 - val_loss: 5.8392 - val_mse: 5.8392\n",
      "Epoch 290/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2177 - mse: 7.2177 - val_loss: 5.9587 - val_mse: 5.9587\n",
      "Epoch 291/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0665 - mse: 7.0665 - val_loss: 5.7900 - val_mse: 5.7900\n",
      "Epoch 292/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1217 - mse: 7.1217 - val_loss: 6.1635 - val_mse: 6.1635\n",
      "Epoch 293/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1713 - mse: 7.1713 - val_loss: 5.9469 - val_mse: 5.9469\n",
      "Epoch 294/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.2899 - mse: 7.2899 - val_loss: 6.0161 - val_mse: 6.0161\n",
      "Epoch 295/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0876 - mse: 7.0876 - val_loss: 5.7501 - val_mse: 5.7501\n",
      "Epoch 296/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.0811 - mse: 7.0811 - val_loss: 6.1300 - val_mse: 6.1300\n",
      "Epoch 297/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.4502 - mse: 7.4502 - val_loss: 5.6192 - val_mse: 5.6192\n",
      "Epoch 298/300\n",
      "226/226 [==============================] - 1s 6ms/step - loss: 7.2002 - mse: 7.2002 - val_loss: 5.6249 - val_mse: 5.6249\n",
      "Epoch 299/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.3751 - mse: 7.3751 - val_loss: 5.6369 - val_mse: 5.6369\n",
      "Epoch 300/300\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 7.1286 - mse: 7.1286 - val_loss: 5.6484 - val_mse: 5.6484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1608d260280>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stage2 모델 학습\n",
    "model_stage2.fit(X_train_stage2,y_train_stage2,validation_split=0.2,  #훈련데이터 내에서 20% 검증데이터로 사용\n",
    "          epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12.27515  ,  6.4108367, 10.771667 , ...,  2.0694313,  3.5150146,\n",
       "         7.9315596],\n",
       "       [12.005274 ,  6.2396727, 10.604524 , ...,  1.9900057,  3.7326427,\n",
       "         7.4503784],\n",
       "       [12.2745   ,  6.4067245, 10.765974 , ...,  2.069982 ,  3.517888 ,\n",
       "         7.9219627],\n",
       "       ...,\n",
       "       [12.2551   ,  6.3526735, 10.694275 , ...,  2.0726485,  3.5600352,\n",
       "         7.7921696],\n",
       "       [12.266847 ,  6.358352 , 10.699018 , ...,  2.076461 ,  3.5516872,\n",
       "         7.8090744],\n",
       "       [12.273808 ,  6.4023476, 10.759915 , ...,  2.0705683,  3.5209463,\n",
       "         7.911748 ]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stage2 모델 예측\n",
    "model_stage2.predict(X_test_stage2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#stage2 모델 정답데이터 만들기\n",
    "#submission의 x데이터 가져오기\n",
    "submission_stage2_X = submission_data.iloc[:,56:70] #stage2의 test 데이터(14개 특성)\n",
    "#submission의 x데이터 전처리(모델에 학습했던 colunm selection 과 같게!)\n",
    "#submission_stage2_X = submission_stage2_X.drop('Machine5.Temperature6.C.Actual',axis=1)\n",
    "#submission의 x데이터 전처리(stage2의 x데이터 처럼 스케일링!)\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(submission_stage2_X)\n",
    "df_sclaed_df = pd.DataFrame(df_scaled)\n",
    "X_stage2_scaler = df_sclaed_df\n",
    "#stage2 모델 정답데이터 출력\n",
    "model_submission2_pre = model_stage2.predict(X_stage2_scaler) #stage2의 정답데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stage2 제출하기\n",
    "#submission 데이터에 정답데이터 넣기\n",
    "#제출데이터 만들기\n",
    "submission_origin = pd.read_csv('./submission_data.csv', index_col=\"time_stamp\")#제출데이터 전처리\n",
    "submission_data = submission_origin.drop(submission_origin.filter(regex='Setpoint').columns,axis=1)\n",
    "submission_data.iloc[:,70:]  = model_submission2_pre\n",
    "submission2 =submission_data.iloc[:,70:]\n",
    "#submission2 저장하고 파일로 만들기\n",
    "import numpy as np\n",
    "np.save('submission2.npy', submission2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stage2.Output.Measurement0.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement1.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement2.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement3.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement4.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement5.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement6.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement7.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement8.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement9.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement10.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement11.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement12.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement13.U.Actual</th>\n",
       "      <th>Stage2.Output.Measurement14.U.Actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_stamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:34</th>\n",
       "      <td>2.012209</td>\n",
       "      <td>1.992933</td>\n",
       "      <td>2.721025</td>\n",
       "      <td>11.143084</td>\n",
       "      <td>3.511558</td>\n",
       "      <td>1.850813</td>\n",
       "      <td>0.358587</td>\n",
       "      <td>1.444426</td>\n",
       "      <td>2.268785</td>\n",
       "      <td>1.695784</td>\n",
       "      <td>1.955846</td>\n",
       "      <td>1.365897</td>\n",
       "      <td>0.528392</td>\n",
       "      <td>2.534656</td>\n",
       "      <td>1.497746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:35</th>\n",
       "      <td>1.773598</td>\n",
       "      <td>1.896771</td>\n",
       "      <td>2.552872</td>\n",
       "      <td>11.003637</td>\n",
       "      <td>3.505978</td>\n",
       "      <td>1.838501</td>\n",
       "      <td>0.347992</td>\n",
       "      <td>1.411851</td>\n",
       "      <td>1.872971</td>\n",
       "      <td>0.944926</td>\n",
       "      <td>1.798144</td>\n",
       "      <td>1.248049</td>\n",
       "      <td>0.487334</td>\n",
       "      <td>2.524044</td>\n",
       "      <td>1.348482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:36</th>\n",
       "      <td>2.371701</td>\n",
       "      <td>2.138803</td>\n",
       "      <td>2.978165</td>\n",
       "      <td>11.366380</td>\n",
       "      <td>3.509716</td>\n",
       "      <td>1.871764</td>\n",
       "      <td>0.373193</td>\n",
       "      <td>1.494250</td>\n",
       "      <td>2.865499</td>\n",
       "      <td>2.703159</td>\n",
       "      <td>2.189040</td>\n",
       "      <td>1.539371</td>\n",
       "      <td>0.589084</td>\n",
       "      <td>2.554048</td>\n",
       "      <td>1.721280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:37</th>\n",
       "      <td>2.645916</td>\n",
       "      <td>2.233885</td>\n",
       "      <td>3.112266</td>\n",
       "      <td>11.321142</td>\n",
       "      <td>3.675618</td>\n",
       "      <td>1.848543</td>\n",
       "      <td>0.406465</td>\n",
       "      <td>1.520062</td>\n",
       "      <td>3.314453</td>\n",
       "      <td>5.493536</td>\n",
       "      <td>2.438739</td>\n",
       "      <td>1.738237</td>\n",
       "      <td>0.654397</td>\n",
       "      <td>2.513266</td>\n",
       "      <td>1.913810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 10:52:39</th>\n",
       "      <td>0.505266</td>\n",
       "      <td>1.358698</td>\n",
       "      <td>1.555845</td>\n",
       "      <td>9.903786</td>\n",
       "      <td>3.754661</td>\n",
       "      <td>1.707840</td>\n",
       "      <td>0.328492</td>\n",
       "      <td>1.218413</td>\n",
       "      <td>-0.241301</td>\n",
       "      <td>0.317639</td>\n",
       "      <td>1.079374</td>\n",
       "      <td>0.732337</td>\n",
       "      <td>0.300734</td>\n",
       "      <td>2.375179</td>\n",
       "      <td>0.591704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:46:57</th>\n",
       "      <td>11.907097</td>\n",
       "      <td>6.217096</td>\n",
       "      <td>10.600464</td>\n",
       "      <td>20.074972</td>\n",
       "      <td>1.298725</td>\n",
       "      <td>2.934066</td>\n",
       "      <td>0.474637</td>\n",
       "      <td>2.973407</td>\n",
       "      <td>18.773403</td>\n",
       "      <td>3.293509</td>\n",
       "      <td>7.446248</td>\n",
       "      <td>5.280759</td>\n",
       "      <td>1.953149</td>\n",
       "      <td>3.786615</td>\n",
       "      <td>7.365846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:09</th>\n",
       "      <td>11.915162</td>\n",
       "      <td>6.218951</td>\n",
       "      <td>10.600798</td>\n",
       "      <td>20.061096</td>\n",
       "      <td>1.313342</td>\n",
       "      <td>2.931102</td>\n",
       "      <td>0.476904</td>\n",
       "      <td>2.973456</td>\n",
       "      <td>18.786247</td>\n",
       "      <td>3.493259</td>\n",
       "      <td>7.457773</td>\n",
       "      <td>5.290481</td>\n",
       "      <td>1.956176</td>\n",
       "      <td>3.782181</td>\n",
       "      <td>7.372791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:14</th>\n",
       "      <td>11.915539</td>\n",
       "      <td>6.219038</td>\n",
       "      <td>10.600813</td>\n",
       "      <td>20.060448</td>\n",
       "      <td>1.314025</td>\n",
       "      <td>2.930963</td>\n",
       "      <td>0.477010</td>\n",
       "      <td>2.973459</td>\n",
       "      <td>18.786846</td>\n",
       "      <td>3.502595</td>\n",
       "      <td>7.458311</td>\n",
       "      <td>5.290936</td>\n",
       "      <td>1.956318</td>\n",
       "      <td>3.781974</td>\n",
       "      <td>7.373115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:15</th>\n",
       "      <td>11.939501</td>\n",
       "      <td>6.224547</td>\n",
       "      <td>10.601804</td>\n",
       "      <td>20.019222</td>\n",
       "      <td>1.357450</td>\n",
       "      <td>2.922156</td>\n",
       "      <td>0.483743</td>\n",
       "      <td>2.973606</td>\n",
       "      <td>18.825005</td>\n",
       "      <td>4.096035</td>\n",
       "      <td>7.492549</td>\n",
       "      <td>5.319818</td>\n",
       "      <td>1.965313</td>\n",
       "      <td>3.768801</td>\n",
       "      <td>7.393747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-06 14:47:19</th>\n",
       "      <td>11.943431</td>\n",
       "      <td>6.225451</td>\n",
       "      <td>10.601967</td>\n",
       "      <td>20.012463</td>\n",
       "      <td>1.364572</td>\n",
       "      <td>2.920712</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>2.973630</td>\n",
       "      <td>18.831261</td>\n",
       "      <td>4.193360</td>\n",
       "      <td>7.498164</td>\n",
       "      <td>5.324555</td>\n",
       "      <td>1.966789</td>\n",
       "      <td>3.766641</td>\n",
       "      <td>7.397130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2818 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Stage2.Output.Measurement0.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             2.012209   \n",
       "2019-03-06 10:52:35                             1.773598   \n",
       "2019-03-06 10:52:36                             2.371701   \n",
       "2019-03-06 10:52:37                             2.645916   \n",
       "2019-03-06 10:52:39                             0.505266   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            11.907097   \n",
       "2019-03-06 14:47:09                            11.915162   \n",
       "2019-03-06 14:47:14                            11.915539   \n",
       "2019-03-06 14:47:15                            11.939501   \n",
       "2019-03-06 14:47:19                            11.943431   \n",
       "\n",
       "                     Stage2.Output.Measurement1.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             1.992933   \n",
       "2019-03-06 10:52:35                             1.896771   \n",
       "2019-03-06 10:52:36                             2.138803   \n",
       "2019-03-06 10:52:37                             2.233885   \n",
       "2019-03-06 10:52:39                             1.358698   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             6.217096   \n",
       "2019-03-06 14:47:09                             6.218951   \n",
       "2019-03-06 14:47:14                             6.219038   \n",
       "2019-03-06 14:47:15                             6.224547   \n",
       "2019-03-06 14:47:19                             6.225451   \n",
       "\n",
       "                     Stage2.Output.Measurement2.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             2.721025   \n",
       "2019-03-06 10:52:35                             2.552872   \n",
       "2019-03-06 10:52:36                             2.978165   \n",
       "2019-03-06 10:52:37                             3.112266   \n",
       "2019-03-06 10:52:39                             1.555845   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            10.600464   \n",
       "2019-03-06 14:47:09                            10.600798   \n",
       "2019-03-06 14:47:14                            10.600813   \n",
       "2019-03-06 14:47:15                            10.601804   \n",
       "2019-03-06 14:47:19                            10.601967   \n",
       "\n",
       "                     Stage2.Output.Measurement3.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                            11.143084   \n",
       "2019-03-06 10:52:35                            11.003637   \n",
       "2019-03-06 10:52:36                            11.366380   \n",
       "2019-03-06 10:52:37                            11.321142   \n",
       "2019-03-06 10:52:39                             9.903786   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            20.074972   \n",
       "2019-03-06 14:47:09                            20.061096   \n",
       "2019-03-06 14:47:14                            20.060448   \n",
       "2019-03-06 14:47:15                            20.019222   \n",
       "2019-03-06 14:47:19                            20.012463   \n",
       "\n",
       "                     Stage2.Output.Measurement4.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             3.511558   \n",
       "2019-03-06 10:52:35                             3.505978   \n",
       "2019-03-06 10:52:36                             3.509716   \n",
       "2019-03-06 10:52:37                             3.675618   \n",
       "2019-03-06 10:52:39                             3.754661   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             1.298725   \n",
       "2019-03-06 14:47:09                             1.313342   \n",
       "2019-03-06 14:47:14                             1.314025   \n",
       "2019-03-06 14:47:15                             1.357450   \n",
       "2019-03-06 14:47:19                             1.364572   \n",
       "\n",
       "                     Stage2.Output.Measurement5.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             1.850813   \n",
       "2019-03-06 10:52:35                             1.838501   \n",
       "2019-03-06 10:52:36                             1.871764   \n",
       "2019-03-06 10:52:37                             1.848543   \n",
       "2019-03-06 10:52:39                             1.707840   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             2.934066   \n",
       "2019-03-06 14:47:09                             2.931102   \n",
       "2019-03-06 14:47:14                             2.930963   \n",
       "2019-03-06 14:47:15                             2.922156   \n",
       "2019-03-06 14:47:19                             2.920712   \n",
       "\n",
       "                     Stage2.Output.Measurement6.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             0.358587   \n",
       "2019-03-06 10:52:35                             0.347992   \n",
       "2019-03-06 10:52:36                             0.373193   \n",
       "2019-03-06 10:52:37                             0.406465   \n",
       "2019-03-06 10:52:39                             0.328492   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             0.474637   \n",
       "2019-03-06 14:47:09                             0.476904   \n",
       "2019-03-06 14:47:14                             0.477010   \n",
       "2019-03-06 14:47:15                             0.483743   \n",
       "2019-03-06 14:47:19                             0.484848   \n",
       "\n",
       "                     Stage2.Output.Measurement7.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             1.444426   \n",
       "2019-03-06 10:52:35                             1.411851   \n",
       "2019-03-06 10:52:36                             1.494250   \n",
       "2019-03-06 10:52:37                             1.520062   \n",
       "2019-03-06 10:52:39                             1.218413   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             2.973407   \n",
       "2019-03-06 14:47:09                             2.973456   \n",
       "2019-03-06 14:47:14                             2.973459   \n",
       "2019-03-06 14:47:15                             2.973606   \n",
       "2019-03-06 14:47:19                             2.973630   \n",
       "\n",
       "                     Stage2.Output.Measurement8.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             2.268785   \n",
       "2019-03-06 10:52:35                             1.872971   \n",
       "2019-03-06 10:52:36                             2.865499   \n",
       "2019-03-06 10:52:37                             3.314453   \n",
       "2019-03-06 10:52:39                            -0.241301   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                            18.773403   \n",
       "2019-03-06 14:47:09                            18.786247   \n",
       "2019-03-06 14:47:14                            18.786846   \n",
       "2019-03-06 14:47:15                            18.825005   \n",
       "2019-03-06 14:47:19                            18.831261   \n",
       "\n",
       "                     Stage2.Output.Measurement9.U.Actual  \\\n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                             1.695784   \n",
       "2019-03-06 10:52:35                             0.944926   \n",
       "2019-03-06 10:52:36                             2.703159   \n",
       "2019-03-06 10:52:37                             5.493536   \n",
       "2019-03-06 10:52:39                             0.317639   \n",
       "...                                                  ...   \n",
       "2019-03-06 14:46:57                             3.293509   \n",
       "2019-03-06 14:47:09                             3.493259   \n",
       "2019-03-06 14:47:14                             3.502595   \n",
       "2019-03-06 14:47:15                             4.096035   \n",
       "2019-03-06 14:47:19                             4.193360   \n",
       "\n",
       "                     Stage2.Output.Measurement10.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              1.955846   \n",
       "2019-03-06 10:52:35                              1.798144   \n",
       "2019-03-06 10:52:36                              2.189040   \n",
       "2019-03-06 10:52:37                              2.438739   \n",
       "2019-03-06 10:52:39                              1.079374   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              7.446248   \n",
       "2019-03-06 14:47:09                              7.457773   \n",
       "2019-03-06 14:47:14                              7.458311   \n",
       "2019-03-06 14:47:15                              7.492549   \n",
       "2019-03-06 14:47:19                              7.498164   \n",
       "\n",
       "                     Stage2.Output.Measurement11.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              1.365897   \n",
       "2019-03-06 10:52:35                              1.248049   \n",
       "2019-03-06 10:52:36                              1.539371   \n",
       "2019-03-06 10:52:37                              1.738237   \n",
       "2019-03-06 10:52:39                              0.732337   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              5.280759   \n",
       "2019-03-06 14:47:09                              5.290481   \n",
       "2019-03-06 14:47:14                              5.290936   \n",
       "2019-03-06 14:47:15                              5.319818   \n",
       "2019-03-06 14:47:19                              5.324555   \n",
       "\n",
       "                     Stage2.Output.Measurement12.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              0.528392   \n",
       "2019-03-06 10:52:35                              0.487334   \n",
       "2019-03-06 10:52:36                              0.589084   \n",
       "2019-03-06 10:52:37                              0.654397   \n",
       "2019-03-06 10:52:39                              0.300734   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              1.953149   \n",
       "2019-03-06 14:47:09                              1.956176   \n",
       "2019-03-06 14:47:14                              1.956318   \n",
       "2019-03-06 14:47:15                              1.965313   \n",
       "2019-03-06 14:47:19                              1.966789   \n",
       "\n",
       "                     Stage2.Output.Measurement13.U.Actual  \\\n",
       "time_stamp                                                  \n",
       "2019-03-06 10:52:34                              2.534656   \n",
       "2019-03-06 10:52:35                              2.524044   \n",
       "2019-03-06 10:52:36                              2.554048   \n",
       "2019-03-06 10:52:37                              2.513266   \n",
       "2019-03-06 10:52:39                              2.375179   \n",
       "...                                                   ...   \n",
       "2019-03-06 14:46:57                              3.786615   \n",
       "2019-03-06 14:47:09                              3.782181   \n",
       "2019-03-06 14:47:14                              3.781974   \n",
       "2019-03-06 14:47:15                              3.768801   \n",
       "2019-03-06 14:47:19                              3.766641   \n",
       "\n",
       "                     Stage2.Output.Measurement14.U.Actual  \n",
       "time_stamp                                                 \n",
       "2019-03-06 10:52:34                              1.497746  \n",
       "2019-03-06 10:52:35                              1.348482  \n",
       "2019-03-06 10:52:36                              1.721280  \n",
       "2019-03-06 10:52:37                              1.913810  \n",
       "2019-03-06 10:52:39                              0.591704  \n",
       "...                                                   ...  \n",
       "2019-03-06 14:46:57                              7.365846  \n",
       "2019-03-06 14:47:09                              7.372791  \n",
       "2019-03-06 14:47:14                              7.373115  \n",
       "2019-03-06 14:47:15                              7.393747  \n",
       "2019-03-06 14:47:19                              7.397130  \n",
       "\n",
       "[2818 rows x 15 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
